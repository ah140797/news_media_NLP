[["index.html", "Taliban Project Overview Software Structure How to read the notebook", " Taliban Project Anders Havbro Hjulmand 2021-12-20 Overview This notebook contains the accompanying code to the exam project for the course Introduction to Cultural Data Science in the autumn of 2021 at the University of Aarhus. This notebook only contains code. Please see the report for the full project. Software The code in this notebook is written in R and Python. I used Rstudio and Jupyter Notebook as IDEs. A lot of packages are used throughout the book. The references to the packages can be found in ?? Structure The structure of the notebook is as follows. In step 1 I acquire the data needed and tidy so it is ready for analysis. In step 2 I make Named Entity Recognition (NER) and plot. In step 3 i make a sentiment analysis and plot. In step 4 i make a LDA topic model and plot. How to read the notebook I have made 4 different highlight colors that i use throughout the book to indicate different things. These colors appear at the beginning of the chapter. div.green { background-color:#93c47d; border-radius: 5px; padding: 20px;} A green color indicates that the chapter (or part of it) was written in Python. div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} A blue color indicates that the code in the chapter is run on multiple datasets, but that the code only shows the pipeline of a single dataset. To run the code on the other dataset, minor changes are needed such as changing paths. div.yellow { background-color:#ffd966; border-radius: 5px; padding: 20px;} A yellow color indicates that the chapter is very similar to another chapter with only minor adjustments. div.red { background-color:#e06666; border-radius: 5px; padding: 20px;} A red color indicates something that the reader should be extra attentive to. "],["scraping-news-articles-with-new-york-times-api.html", "Chapter 1 Scraping News Articles with New York Times API", " Chapter 1 Scraping News Articles with New York Times API First thing first, we start by loading the packages tidyverse (Wickham et al. 2019) and jsonlite (Ooms 2014). pacman::p_load(tidyverse, jsonlite) New York Times has developed a public API which makes it fairly straightforward to scrape news articles from their database. The following link will get you started: https://developer.nytimes.com/. I used the Article Search API to search news articles. I only wanted to include articles containing the word Taliban and thus entered Taliban in the q query parameter. The result of the query can be found in the URL below. #defining the url as a string. url &lt;- &quot;https://api.nytimes.com/svc/search/v2/articlesearch.json?q=Taliban&amp;api-key=vsGvjCtUXFjiKDlAUQgubKUhe7MjVWJR&quot; Clicking the url above will open up a JSON-file. Luckily the package called jsonlite can handle json-files very effectively, converting them into a nicely formated dataframe with rows as observations and coloumns as variables. We first make a dataframe called initial which contains 10 observations, i.e. 10 news articles and 25 variables such as title and abstract. #loading the JSON file as a dataframe initial &lt;- fromJSON(url) %&gt;% data.frame() #checking dimensions dim(initial) ## [1] 10 25 #checking column names colnames(initial) %&gt;% knitr::kable(caption = &quot;Column names of the initial dataset&quot;, col.names = &quot;Column Names&quot;) (#tab:checking output from 10 articles)Column names of the initial dataset Column Names status copyright response.docs.abstract response.docs.web_url response.docs.snippet response.docs.lead_paragraph response.docs.print_section response.docs.print_page response.docs.source response.docs.multimedia response.docs.headline response.docs.keywords response.docs.pub_date response.docs.document_type response.docs.news_desk response.docs.section_name response.docs.subsection_name response.docs.byline response.docs.type_of_material response.docs._id response.docs.word_count response.docs.uri response.meta.hits response.meta.offset response.meta.time Using the search terms above gives us 23.500 articles in total, but the dataframe initial only contains 10 articles. Why is that? When requesting news articles from the API, it is important to note that it only returns 10 results at a time, even though there are many more results. This is like searching on Google where each page contains a limited number of results. When making a query on the New York Times API it returns 10 results per page. Now we need to find out how many pages there are in total. To do so we divide the total number of articles by 10 and subtract 1. The total number of pages is 2350. total_pages &lt;- round((initial$response.meta.hits[1] / 10)-1) total_pages ## [1] 2359 Next up I create a for loop to iterate through all the pages and parse information from each page. This is the same process as making initial but scaled up to get a dataframe containing all 23.500 articles. #making an empty list which dataframes can be appended to pages &lt;- list() #Beginning the for-loop for(i in 0:total_pages){ #The same function as earlier is used with a minor tweak that appends the page-number #to the end of the url. article &lt;- fromJSON(paste0(url, &quot;&amp;page=&quot;, i), flatten = TRUE) %&gt;% data.frame() #Printing a message so I know if all is good message(&quot;Retrieving page &quot;, i) #appending the dataframe &quot;article&quot; to the list &quot;pages&quot; pages[[i+1]] &lt;- article #New York Times have set a limit on 10 requests per minute. That equals 6 #seconds of sleep between requests. Sys.sleep(6) } The for loop a list called pages which contains 2350 dataframes each containing 10 articles. The function rbind_pages is used to combine the list of dataframes into a single dataframe called NYT_raw. Now we have scraped 23.502 articles and can continue to data cleaning.1 #rbind_pages is used to combine a list of dataframes into a single dataframe NYT_raw &lt;- rbind_pages(pages) References "],["cleaning-the-data.html", "Chapter 2 Cleaning The Data", " Chapter 2 Cleaning The Data Now it is time to clean up the dataframe NYT_raw. First thing first, we start by loading the packages tidyverse (Wickham et al. 2019) andDT (Xie, Cheng, and Tan 2021). pacman::p_load(tidyverse, DT) Here we are going to make the cleaned dataset NYT_clean also refered to as df_NYT from the dataset NYT_raw. It mostly follows the syntax from the package tidyverse. #selecting columns to keep coloumns_to_select &lt;- c(&quot;response.docs.headline.main&quot;, &quot;response.docs.web_url&quot;, &quot;response.docs.pub_date&quot;) #creating the new dataframe df_NYT &lt;- NYT_raw %&gt;% #selecting the defined columns select(coloumns_to_select) %&gt;% #renaming columns to more humane names rename( &quot;headline&quot; = &quot;response.docs.headline.main&quot;, &quot;url&quot; = &quot;response.docs.web_url&quot;, &quot;date&quot; = &quot;response.docs.pub_date&quot;, ) %&gt;% #formatting columns to the correct class mutate( date = as.Date(date) ) %&gt;% #arranging by date so that the articles are in chronological order arrange(by=date) #saving to a csv write_csv(df_NYT, &quot;data/new_york_times/data_additional/NYT_clean_cp2.csv&quot;) Now lets inspect the cleaned dataframe. to see if it looks allright. To do this we use the function datatable from the package DT. The cleaned dataset contains the following columns: headline which is the title/headline of the article url which is a url leading to the the article on the NYT webpage date which is the publication date #reading the csv df_NYT &lt;- read_csv(&quot;data/new_york_times/data_additional/NYT_clean_cp2.csv&quot;) #making a nice dataframe that we can browse. Note that we remove abstract because there is too much text in it to show in a nice way. font.size &lt;- &quot;8pt&quot; DT::datatable( df_NYT, rownames = FALSE, filter = &quot;top&quot;, options = list( initComplete = htmlwidgets::JS( &quot;function(settings, json) {&quot;, paste0(&quot;$(this.api().table().container()).css({&#39;font-size&#39;: &#39;&quot;, font.size, &quot;&#39;});&quot;), &quot;}&quot;), pagelength = 3, scrollX=T, autoWidth = TRUE ) ) References "],["scrape-bread-text-of-articles.html", "Chapter 3 Scrape bread text of articles 3.1 Scraping in Python 3.2 Inspecting the bread text", " Chapter 3 Scrape bread text of articles div.blue { background-color:#93c47d; border-radius: 5px; padding: 20px;} This chapter is written in Python. To see the original file go to the folder python_scripts/. 3.1 Scraping in Python The cleaned dataframe df_NYT contains a coloumn called url. Clicking on this url leads us to the article on New York Times. Now we are going to move into Python to scrape the breadtext from the URLs. There is a package called BeautifulSoup which makes this less painful to do. That is why i use Python here. The chunk below loads packages and the data. It also sets up variables for the iteration. from bs4 import BeautifulSoup import requests import pandas as pd #importing the cleaned data df_NYT = pd.read_csv(&quot;data/new_york_times/data_additional/NYT_clean_cp2.csv&quot;) The chunk below is where the work takes place. It first sets up variables for the iteration. The for-loop iterates through all the urls in the dataframe, makes a soup which is basically the html for that url. It then looks through the soup to find the tag p and the class css-axufdj evys1bk0. All the breadtext exist within this tag and class. I learned this by opening up an articles in Google Chrome and inspecting the html code by right-clicking and choosing Inspect. Then it appends the bread text to a list. Lastly it writes a new column to the dataframe containing the bread text for each article and overwrites the old dataframe. #making an empty list where the breadtext from all articles can be appended to all_bread = [] #making an index to print in the for-loop i = 0 #interate through each url in df, scrape the breadtext and appending it to the list all_bread for url in df_NYT[&#39;url&#39;]: # setting up empty string bread_article = &quot;&quot; #sending request for url page = requests.get(url) #creating soup (lingo from the package beautifulsoup) soup = BeautifulSoup(page.text, &quot;lxml&quot;) #finding the breadtext using beautifulsoup for pre_bread in soup.find_all(&quot;p&quot;, class_=&quot;css-axufdj evys1bk0&quot;): bread = pre_bread.text bread_article += &quot; &quot; + bread all_bread.append(bread_article) i += 1 print(&quot;Scraping article &quot;, i, &quot;/&quot;, len(df_NYT.index)) #making a new column in df containing all the breadtext for each observation respectively df_NYT[&#39;bread_text&#39;] = all_bread #saving the df df_NYT.to_csv(&quot;data/new_york_times/data_additional/NYT_clean.csv&quot;, index = False) The dataset is 148 Mb large which is actually too large for Github, that has a file limit on 100 MB. So here we just split the data into two datasets at 74 Mb each. #reading the full dataset df_NYT &lt;- read_csv(&quot;data/new_york_times/data_additional/NYT_clean.csv&quot;) #splitting df_NYT_1 &lt;- df_full[1:11750,] df_NYT_2 &lt;- df_full[11751:23500,] #saving the new smaller datasets write_csv(df_NYT_1, &quot;data/new_york_times/data_additional/NYT_clean_1_cp3.csv&quot;) write_csv(df_NYT_2, &quot;data/new_york_times/data_additional/NYT_clean_2_cp3.csv&quot;) 3.2 Inspecting the bread text We load the data from the two smaller datasets. df1 &lt;- read_csv(&quot;data/new_york_times/data_additional/NYT_clean_1_cp3.csv&quot;) df2 &lt;- read_csv(&quot;data/new_york_times/data_additional/NYT_clean_1_cp3.csv&quot;) df_NYT &lt;- rbind(df1,df2) df_NYT &lt;- as_tibble(df_NYT) All right. Lets see how the new coloumn looks when we load it into R. It is a wall of text - not very readable for a human, but that is quite allright. #printing the breadtext from the 11th. article. c(df_NYT[11,4]) ## $bread_text ## [1] &quot;THEY ARE EVERYWHERE IN the ruins of Kabul, scavenging, playing, begging. In a city that has shrunk from two million in the last days of the Soviet occupation, in 1989, to about 600,000, children seem at times to make up the bulk of the population. Too young to fight in one of Afghanistan&#39;s many warlord armies, orphaned or born to families too stubborn or poor to flee, they scratch out a bare existence in a moonscape left after 15 months of pounding by the rockets and shells of rival Afghan factions. Through the centuries and numerous invasions by foreign powers, Kabul was never sacked. Today, half the city lies in ruins, including the Arg Palace, once the principal residence of Afghanistan&#39;s kings, the Id Mosque, the central place of worship, and a historic fortress, the Bala Hisar. On either side of the Kabul River, which cuts the city in two, whole neighborhoods have been demolished. The human suffering is worse. More than a million Afghans died during the Soviet occupation. Many thousands more perished in the bombardment of Kabul. That pounding broke off in February after the Taliban, an army led by fundamentalist clerics, routed another hard-line faction that had been shelling the city, only to resume the shelling itself until it was driven back from the city by the forces of President Burhanuddin Rabbani. Even so, these are some of the best times many of these children can remember; if there is little food and no fuel for heat, at least there are days, and sometimes whole weeks, with only occasional bombs and rockets, and no troops looting, pillaging and raping, as they did in 1992. The great fear is that the relative tranquillity of the last few weeks may be shattered by a last great battle for outright victory between the Taliban and Rabbani forces. As these photographs attest, the orphans and other children of Kabul accept their fate with stoicism and courage. It is, of course, depressing to think of a generation of Afghans for whom scavenging all day for firewood is as routine as hanging out in the mall. In this barren terrain, they can barely look even for the smallest joys -- a kite rising into a fresh breeze, a game of tag. The recent lulls in the fighting have brought a welcome influx of fresh produce and other goods to Kabul&#39;s bare markets, but few people can afford them. Doctors say that as much as 60 percent of the city&#39;s population is undernourished, and bronchial infections and skin diseases are rampant. While the young may not suffer any more than anyone else, they personify the horror of a country that has plunged to the meanest level of subsistence. This, you keep telling yourself, is the only life they know.&quot; We also check how many articles were failed to scrape. That number is 2.346 which is ~ 10% of all the articles. #checking how many articles failed to scrape sum(is.na(df_NYT$bread_text)) ## [1] 730 Lets inspect these missing articles a bit further. Do they come from a specific time period? #making a dataframe containing the sum of missing articles for each date y1 &lt;- df_NYT %&gt;% filter(is.na(bread_text)) %&gt;% group_by(date) %&gt;% tally() %&gt;% rename(missing_articles = n) %&gt;% ungroup() #making a dataframe containing the sum of articles for each date y2 &lt;- df_NYT %&gt;% filter(!is.na(bread_text)) %&gt;% group_by(date) %&gt;% tally() %&gt;% rename(articles = n) %&gt;% ungroup() #joining the two dataframes together and plotting articles and missing articles a ton top of each other according to date. left_join(y2, y1) %&gt;% pivot_longer( cols = c(&quot;missing_articles&quot;, &quot;articles&quot;), names_to = &quot;article_type&quot; ) %&gt;% ggplot() + aes(x=date, y= value, color = article_type) + geom_line() + labs(y = &quot;hits&quot;) Figure 3.1: Timeline of the number of missing articles and compared to intact articles. Eyeballing figure 3.1 it is the case that the missing articles stems from certain time periods around 2001 and especially around 2009-2010. The implications for the further analysis is that there will be some weak points in the timeline, where the number of articles to be analyzed are a bit sparse. Furthermore, some valueable articles might be missing at crucial points in the timeline. However, there are still not any points in the timeline where the number of articles drops completely, so that is good. So we save two new dataframes where the articles with missing bread text are filtered out. Our new dataframe has 21.154 rows corresponding to 21.154 articles. #filtering out missing articles df_NYT &lt;- df_full %&gt;% filter(!is.na(bread_text)) #splitting df_NYT_1 &lt;- df_full[1:10577,] df_NYT_2 &lt;- df_full[10578:21154,] #saving the new smaller datasets write_csv(df_NYT_1, &quot;data/new_york_times/NYT_clean_1.csv&quot;) write_csv(df_NYT_2, &quot;data/new_york_times/NYT_clean_2.csv&quot;) "],["scraping-articles-form-the-guardian.html", "Chapter 4 Scraping articles form the Guardian", " Chapter 4 Scraping articles form the Guardian div.blue { background-color:#ffd966; border-radius: 5px; padding: 20px;} This chapter is very similar to chapter 1. Here all the steps are just adjusted to the fit The Guardians API. First thing first, we start by loading the packages tidyverse (Wickham et al. 2019) and jsonlite (Ooms 2014). pacman::p_load(tidyverse, jsonlite) The Guardian has developed a public API which makes it fairly straightforward to scrape news articles from their database. The following link will get you started: https://open-platform.theguardian.com/access/. I only wanted to include articles containing the word Taliban and thus entered Taliban in the query parameter. I also ordered articles by oldest. The result of the query can be found in the URL below. #defining the url as a string. url &lt;- &quot;https://content.guardianapis.com/search?order-by=oldest&amp;q=Taliban&amp;api-key=778d99ad-d154-42e6-ade5-eeb653baf011&quot; Clicking the url above will open up a JSON-file. Luckily the package called jsonlite can handle json-files very effectively, converting them into a nicely formated dataframe with rows as observations and coloumns as variables. We first make a dataframe called initial which contains 10 observations, i.e. 10 news articles and 25 variables such as title and url. #loading the JSON file as a dataframe initial &lt;- fromJSON(url) %&gt;% data.frame() #checking dimensions dim(initial) ## [1] 10 19 #checking column names colnames(initial) %&gt;% knitr::kable(caption = &quot;Column names of the initial dataset&quot;, col.names = &quot;Column Names&quot;) (#tab:checking output from 10 articles2)Column names of the initial dataset Column Names response.status response.userTier response.total response.startIndex response.pageSize response.currentPage response.pages response.orderBy response.results.id response.results.type response.results.sectionId response.results.sectionName response.results.webPublicationDate response.results.webTitle response.results.webUrl response.results.apiUrl response.results.isHosted response.results.pillarId response.results.pillarName Using the search terms above gives us 16.432 articles in total, but the dataframe initial only contains 10 articles. Why is that? When requesting news articles from the API, it is important to note that it only returns 10 results at a time, even though there are many more results. This is like searching on Google where each page contains a limited number of results. When making a query on the New York Times API it returns 10 results per page. Now we need to find out how many pages there are in total. To do so we divide the total number of articles by 10 and subtract 1. The total number of pages is 1644. total_pages &lt;- round((initial$response.total[1] / 10)) total_pages ## [1] 1652 Next up I create a for loop to iterate through all the pages and parse information from each page. This is the same process as making initial but scaled up to get a dataframe containing all 16.432 articles. #making an empty list which dataframes can be appended to pages &lt;- list() #Beginning the for-loop for(i in 1:total_pages){ #The same function as earlier is used with a minor tweak that appends the page-number #to the end of the url. article &lt;- fromJSON(paste0(url, &quot;&amp;page=&quot;, i), flatten = TRUE) %&gt;% data.frame() #Printing a message so I know if all is good message(&quot;Retrieving page &quot;, i) #appending the dataframe &quot;article&quot; to the list &quot;pages&quot; pages[[i+1]] &lt;- article #tHe GUardian have set a limit on 12 requests per minute. That equals 5 #seconds of sleep between requests. Sys.sleep(5) } The for loop a list called pages which contains 1645 dataframes each containing 10 articles. The function rbind_pages is used to combine the list of dataframes into a single dataframe called guardian_raw. Now we have scraped 16.432 articles and can continue to data cleaning. #rbind_pages is used to combine a list of dataframes into a single dataframe guardian_raw &lt;- rbind_pages(pages) References "],["cleaning-the-data-1.html", "Chapter 5 Cleaning the Data", " Chapter 5 Cleaning the Data div.blue { background-color:#ffd966; border-radius: 5px; padding: 20px;} Note: This chapter is very similar to chapter 5. Here all the steps are just adjusted to the fit the dataset from The Guardian- First thing first, we start by loading the packages tidyverse (Wickham et al. 2019) andDT (Xie, Cheng, and Tan 2021) pacman::p_load(tidyverse, DT) Now it is time to clean up the dataframe guardian_raw that we made in chapter @ref In this chunk we are going to make the cleaned dataset guardian_clean also refered to as df_guardian from the dataset guardian_raw. It mostly follows the syntax from the package tidyverse. #selecting columns to keep coloumns_to_select &lt;- c(&quot;response.results.webTitle&quot;, &quot;response.results.webUrl&quot;, &quot;response.results.webPublicationDate&quot;) #creating the new dataframe df_guardian &lt;- guardian_raw %&gt;% #selecting the defined columns select(coloumns_to_select) %&gt;% #renaming columns to more humane names rename( &quot;headline&quot; = &quot;response.results.webTitle&quot;, &quot;url&quot; = &quot;response.results.webUrl&quot;, &quot;date&quot; = &quot;response.results.webPublicationDate&quot;, ) %&gt;% #formatting columns to the correct class mutate( date = as.Date(date), ) %&gt;% #arranging by date so that the articles are in chronological order arrange(by=date) #saving to a csv write_csv(df_guardian, &quot;data/guardian/data_additional/guardian_clean_cp2.csv&quot;) Now lets inspect the cleaned dataframe. to see if it looks allright. To do this we use the function datatable from the package DT. #reading the csv df_guardian &lt;- read_csv(&quot;data/guardian/data_additional/guardian_clean_cp2.csv&quot;) #making a nice dataframe that we can browse. font.size &lt;- &quot;8pt&quot; DT::datatable( df_guardian, rownames = FALSE, filter = &quot;top&quot;, options = list( initComplete = htmlwidgets::JS( &quot;function(settings, json) {&quot;, paste0(&quot;$(this.api().table().container()).css({&#39;font-size&#39;: &#39;&quot;, font.size, &quot;&#39;});&quot;), &quot;}&quot;), pagelength = 3, scrollX=T, autoWidth = TRUE ) ) The cleaned dataset contains the following columns: headline which is the title/headline of the article url which is a url leading to the the article on The Guardian webpage date which is the publication date All Guchi. References "],["scrape-bread-text-of-articles-1.html", "Chapter 6 Scrape bread text of articles 6.1 Scraping in Python 6.2 Inspecting the bread text", " Chapter 6 Scrape bread text of articles div.green { background-color:#93c47d; border-radius: 5px; padding: 20px;} This chapter is written in Python. To see the original file go to the folder python_scripts/. div.yellow { background-color:#ffd966; border-radius: 5px; padding: 20px;} Note: This chapter is very similar to chapter 6. Here all the steps are just adjusted to the fit the dataset from The Guardian- 6.1 Scraping in Python The cleaned dataframe df_guardian contains a coloumn called url. Clicking on this url leads us to the article on The Guardian. Now we are going to move into Python to scrape the breadtext from the URLs. There is a package called BeautifulSoup which makes this less painful to do. That is why i use Python here. The chunk below loads packages and the data. from bs4 import BeautifulSoup import requests import pandas as pd #importing the cleaned data df_guardian = pd.read_csv(&quot;data/guardian/data_additional/guardian_clean_cp2.csv&quot;) The chunk below is where the work takes place. It first sets up variables for the iteration. The for-loop iterates through all the urls in the dataframe, makes a soup which is basically the html for that url. Then there is something unfortunate. The bread texts of the articles does not appear under the same class as was the case with the articles from NYT. Instead the bread texts appear under multiple classes. I define these classes, loop through them to find the correct one and insert the correct class in the function find_all.Then it appends the bread text to a list. Lastly it writes a new column to the dataframe containing the bread text for each article and overwrites the old dataframe. #making an empty list where the breadtext from all articles can be appended to all_bread = [] #making an index to print in the for-loop i = 0 #interate through each url in df, scrape the breadtext and appending it to the list all_bread for url in df_guardian[&#39;url&#39;]: # setting up empty string bread_article = &quot;&quot; #sending request for url page = requests.get(url) #creating soup (lingo from the package beautifulsoup) soup = BeautifulSoup(page.text, &quot;lxml&quot;) #breadtext appears under more than one tag. So here we define these tags. tags = [&#39;dcr-o5gy41&#39;,&#39;dcr-t0ikv9&#39;,&#39;dcr-bixwrd&#39;] #specify the tag where the bread text is. soup = str(soup) for tag in tags: if tag in soup: correct_tag = tag print(correct_tag) #defining soup again soup = BeautifulSoup(page.text, &quot;lxml&quot;) #finding the breadtext using beautifulsoup and the correct tag for pre_bread in soup.find_all(&quot;p&quot;, class_=correct_tag): bread = pre_bread.text bread_article += &quot; &quot; + bread all_bread.append(bread_article) i += 1 print(&quot;Scraping article &quot;, i, &quot;/&quot;, len(df_guardian.index)) #making a new column in df containing all the breadtext for each observation respectively df_guardian[&#39;bread_text&#39;] = all_bread 6.2 Inspecting the bread text We load the data into R. df_guardian &lt;- read_csv(&quot;data/guardian/data_additional/guardian_clean_cp3.csv&quot;) All right. Lets see how the new coloumn looks when we load it into R. It is a wall of text - not very readable for a human, but that is quite allright. #printing the breadtext from the 16. article. c(df_guardian[16,4]) ## $bread_text ## [1] &quot;Long-frozen relations between Britain and Iran are thawing rapidly as fellow Europeans and the United States queue up to woo a regime only recently shunned as a pariah. Despite the unresolved Salman Rushdie affair, the Anglo-Iranian honeymoon is blossoming. A top Iranian foreign ministry official is due in London next week and a ground-breaking British ministerial visit to Tehran is possible later this year. Ali Ahani is paying a return visit after John Shepherd, the third most senior diplomat in the Foreign Office, held discreet talks in Tehran in July. Mr Ahani is the most senior Iranian official to come to Britain in a decade. One immediate prospect is a meeting between Robin Cook, the Foreign Secretary, and his Iranian counterpart, Kamal Kharrazi, at the United Nations General Assembly later this month. Britain wants to build on President Mohammed Khatami&#39;s positive attitude to the West and to encourage him in his struggle against hardline opponents. It is anxious not be left behind in trade and investment, especially in the energy sector, by less reticent European competitors finally free of the threat of US sanctions. Officials say that with the US now actively seeking improved ties after years of enmity, Tony Blair wants Britain to follow suit. Britain would like the Iranian regime to distance itself from the Rushdie fatwa. Tehran has given verbal assurances that no one will be sent to murder the author of the Satanic Verses, but insists that the edict cannot be annulled. One possibility is for the government to call for the bounty offered by a religious foundation to be dropped. \\&quot;They know what we want,\\&quot; a senior FO source said. Mr Rushdie&#39;s supporters are lobbying for a public meeting between the novelist and the Prime Minister to underline the Government&#39;s commitment to his safety. Derek Fatchett, the minister responsible for the Middle East, hopes to visit Tehran, though probably not in time for the city&#39;s international trade fair in October. Given public progress on Mr Rushdie, diplomatic relations could be upgraded to ambassadorial level. Plans are under way to expand the British mission and revive cultural activities by the British Council, which has been absent from Iran since the 1979 revolution. In recent weeks Britain has renewed short-insurance cover for exports to Iran and promised medium-term cover by the end of the year. Mr Fatchett gave a rare interview to the official Iranian news agency. And the FO issued a strong condemnation of terrorism after the opposition mojahedin claimed responsibility for assassinating the former head of the country&#39;s prisons&#39; administration. \\&quot;This really illustrates the extent of the concessions the Foreign Office is making to appease the mullahs,\\&quot; a mojahedin spokesman said. Italy and France have both sent senior ministers to Tehran, and Germany&#39;s foreign minister, Klaus Kinkel, said he could envisage making a visit in the near future. Last year Germany and all other EU members recalled their ambassadors from Tehran in protest when a German court ruled that senior Iranian leaders had ordered the 1992 killings of Kurdish dissidents in Berlin. Since then improved ties with the West have been made possible by Tehran&#39;s dislike of the Taliban regime in Afghanistan, its helpful role in central Asia and the Gulf, and its less militant stance on the Arab-Israeli peace process. President Khatami and his supporters are said to have distanced themselves from state terrorism, but concern about the country&#39;s long-term nuclear ambitions remains.&quot; We also check how many articles were failed to scrape. That number is 6.826 which is ~ 40% of all the articles. This is quite a lot, but we still have 9609 articles left. #checking how many articles failed to scrape sum(is.na(df_guardian$bread_text)) ## [1] 6826 Lets inspect these missing articles a bit further. Do they come from a specific time period? #making a dataframe containing the sum of missing articles for each date y1 &lt;- df_guardian %&gt;% filter(is.na(bread_text)) %&gt;% group_by(date) %&gt;% tally() %&gt;% rename(missing_articles = n) %&gt;% ungroup() #making a dataframe containing the sum of articles for each date y2 &lt;- df_guardian %&gt;% filter(!is.na(bread_text)) %&gt;% group_by(date) %&gt;% tally() %&gt;% rename(articles = n) %&gt;% ungroup() #joining the two dataframes together and plotting articles and missing articles a ton top of each other according to date. left_join(y2, y1) %&gt;% pivot_longer( cols = c(&quot;missing_articles&quot;, &quot;articles&quot;), names_to = &quot;article_type&quot; ) %&gt;% ggplot() + aes(x=date, y= value, color = article_type) + geom_line() + labs(y = &quot;hits&quot;) Figure 6.1: Timeline of the number of missing articles compared to intact articles. Eyeballing figure 6.1 it is the case that the missing articles stems from certain time periods around 2001 and also around 2009-2010. The implications for the further analysis is that there will be some weak points in the timeline, where the number of articles to be analyzed are a bit sparse. Furthermore, some valueable articles might be missing at crucial points in the timeline. However, there are still not any points in the timeline where the number of articles drops completely, so that is good. Finally we save a new dataframe where the articles with missing bread text are filtered out. Our new dataframe has 9.609 rows corresponding to 9.609 articles. #filtering out missing articles df_guardian &lt;- df_guardian %&gt;% filter(!is.na(bread_text)) #saving write_csv(df_guardian, &quot;data/guardian/guardian_clean.csv&quot;) "],["preprocess-breadtext-for-both-datasets.html", "Chapter 7 Preprocess Breadtext for both Datasets", " Chapter 7 Preprocess Breadtext for both Datasets div.green { background-color:#93c47d; border-radius: 5px; padding: 20px;} This chapter is written in Python. To see the original file go to the folder python_scripts/. div.blue { background-color:#76a5af; border-radius: 5px; padding: 20px;} All the code in the chapter is run on both df_NYT and df_guardian but here i only show the processing df_NYT. Now we are going to move into the domain of Natural Language Processing (NLP). This is a huge field with many applications and many methods/analysis. Moreover there are a ton of packages in both R and Python for doing NLP. I have chosen to use a package in python called spacy. This package is really powerful and has great documentation. See their webpage to get started: https://spacy.io/. So, going back to the project we have scraped the breadtext from all the articles. We need to preprocess that text before we can apply NLP-analysis. We also use spacy to preprocess the text because it has some useful functions. The preprocessing contains three major steps: We remove punctuation and special characters. We remove stopwords. Stopwords are those common/neutral words that do not add meaning to a text. These are words such as allow or a. We lemmatize all the words. Lemmatization is the process of returning all words to their most basic form, the lemma. For example, the verb to walk may appear as walk, walked or walking. Lemmatization returns all these forms of walking to their base form to walk. All right, lets see some code. We run all the code below for both df_NYT and df_guardian, but I only show the code for df_NYT. We start by loading packages and data. import spacy import numpy as np import pandas as pd #importing the stopwords from spacy.lang.en.stop_words import STOP_WORDS #loading the dataframes and combining them df1 = pd.read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 = pd.read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) #combining df = pd.concat([df1, df2]) Then we load what is called the english small pipeline from spacy. This is basicly a nlp-model with lots of cool functions that we are going to use. We also exclude certain components of the pipeline that we dont need to speed up processing. nlp = spacy.load(&quot;en_core_web_sm&quot;, exclude=[&quot;ner&quot;, &quot;entity_linker&quot;, &quot;entity_ruler&quot;]) Now we can define helper functions that performs the preprocessing steps described above. We first define a function that removes punctuation, special characters and stopwords. It takes article as argument which is one breadtext from an article. It then converts that article to an nlp-object which is necesarry for the functions to work. It then looks through all the tokens (which is basically all the words) and filters out special characters, stopwords and extra spaces. # function for Removing punctuation, stopwords and special chars from a sentence using spaCy def remove_special_chars_and_stopwords(article): article = nlp(article) article = &#39; &#39;.join([token.text for token in article if token.is_punct != True and token.is_quote != True and token.is_bracket != True and token.is_currency != True and token.is_digit != True and token.is_stop != True]) #removing extra spaces article = &quot; &quot;.join(article.split()) return nlp(article) We define another helper function which performs the lemmatizing. It looks through all the words and converts them to their lemma. #function for lemmatizing def lemmatize(sentence): article = &#39; &#39;.join([word.lemma_ for word in sentence]) return nlp(article) Now, lets see how these functions work on a small snippet of text from an article. We first print the original text, then the text after stopwords and punctuation are removed and finally after lemmatization is applied. We see how the text becomes shorter and less readable for us mere humans when these steps are applied. But for the functions in spacy the last text is just yummi. #trying out the function on a single text article = df.iloc[28,3] print(&quot;Orignial Text: &quot;) print(article[0:582] + &quot;\\n&quot;) #removing special chars and stopwords article = remove_special_chars_and_stopwords(article) print(&quot;Stopwords and punctuation removed: &quot;) print(str(article)[0:405] + &quot;\\n&quot;) #lemmatizing the article article = lemmatize(article) print(&quot;Lemmatization applied:&quot;) print(str(article)[0:362] + &quot;\\n&quot;) We now know that the helper functions do what they are supposed to do. This means that we can apply those functions to all the articles in a for-loop. #defining index to loop over article_index = 0 #defining lists for the cleaned articles articles_clean = [] #looping over bread texts from all articles and running the functions on them for article in df.index: #load article using index article = df.iloc[article_index,3] #cleaning the article article = remove_special_chars_and_stopwords(article) #lemmatizing the article article = lemmatize(article) #appending the clean article to a list of clean articles articles_clean.append(article) #indexing to next article article_index += 1 #checking progress print(article_index) #making a new column in df containing the cleaned bread text df[&#39;bread_text_preprocessed&#39;] = articles_clean Lastly we save it to three datasets. Otherwise the files are too large for Git. #splitting into three datasets df1 = df.iloc[0:7500,] df2 = df.iloc[7501:15000,] df3 = df.iloc[15001:21113, ] #saving to three new files df1.to_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;, index = False) df2.to_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;, index = False) df3.to_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;, index = False) All good. We are now done with cleaning, wrangling and preprocessing. We are ready for analysis! "],["overview-1.html", "Overview Structure", " Overview Named entity recognition (NER) is an NLP-technique used to classify entities into categories such as persons, locations, verbs etc. The NER in spaCy has been trained on a corpus and learned to recognize these entities and classify them correctly. However, it does not pick up all instances of a person or location because it is only as good as the corpus it was trained on. But still, we are going to help it a little bit. There are many categories of entities. We are interested the following entities. Most of them are self-explanatory. Others i will explain: nouns verbs adjectives GPEs which is short for Geo-Political-Entity, i.e. countries, cities, states. *persons We are also going to look for the entities below as an exploratory exercise, to see if we can find something interesting. But they wont be used for later analysis. FACs which buildings, airports etc. ORGs which is organizations Structure In chapter 8 I will head into python to extract all the entities from the categories described above. In chapter 9 I make a map of the world showing how different parts of the world were engaged in the Taliban conflict at different points in time. For this we need GPEs. The end product is going to be two vizualisations, basically showing the same trends but in different formats: A plotly interactive map where the user can also browse through time periods. A shiny app where the user can browse through time periods. In chapter 10 i want to extract key persons from the Taliban conflict and make timelines showing how different persons were active at different points in time of the conflict. In chapter 11 i want to extract key organisations from the Taliban conflict and make timelines showing how different organisations were active at different points in time of the conflict. In chapter 12 I want to extract important words, nouns, verbs and adjecives and make timelines. I will use these timelines to make some general claims about the conflict. "],["extracting-ners.html", "Chapter 8 Extracting NERs", " Chapter 8 Extracting NERs div.green { background-color:#93c47d; border-radius: 5px; padding: 20px;} This chapter is written in Python. To see the original file go to the folder python_scripts/. div.blue { background-color:#76a5af; border-radius: 5px; padding: 20px;} All the code in the chapter is run on both df_NYT and df_guardian but here i only show the processing df_NYT. Lets move into the code. We load up packages, data and the small english model. import spacy import numpy as np import pandas as pd from collections import Counter #loading the dataframes and combining them df1 = pd.read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 = pd.read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) df3 = pd.read_csv((&quot;data/new_york_times/NYT_clean_3.csv&quot;) #combining df = pd.concat([df1, df2, df3]) #loading small english model nlp = spacy.load(&quot;en_core_web_sm&quot;) Next up we define helper functions that pick up entities and assign them to a category, e.g. person or Organisation (ORG) There are a lot of functions below but really it is just repetition of the same procedure: The functions takes an article and filters out entities from the desired category (e.g. VERB or GPE). It then adds a date-stamp to them. Lastly it makes a counter-object which is a special type of object data that groups values by unique names and counts the number of instances. Lets go bby. #-------------functions for counting the number of instances of different objects--------------------- #Nouns def noun_freq(article): nouns = [token.text for token in article if token.pos_ == &quot;NOUN&quot;] new_nouns = [] for noun in nouns: noun = noun + &quot;_&quot; + Date noun = noun + &quot;_&quot; + str(article_index) new_nouns.append(noun) noun_freq = Counter(new_nouns) return(noun_freq) #Verbs def verb_freq(article): verbs = [token.text for token in article if token.pos_ == &quot;VERB&quot;] new_verbs = [] for verb in verbs: verb = verb + &quot;_&quot; + Date verb = verb + &quot;_&quot; + str(article_index) new_verbs.append(verb) verb_freq = Counter(new_verbs) return(verb_freq) #Adjectives def adjective_freq(article): adjectives = [token.text for token in article if token.pos_ == &quot;ADJ&quot;] new_adjectives = [] for adjective in adjectives: adjective = adjective + &quot;_&quot; + Date adjective = adjective + &quot;_&quot; + str(article_index) new_adjectives.append(adjective) adjective_freq = Counter(new_adjectives) return(adjective_freq) #GPE def GPE_freq(article): GPEs = [token.text for token in article if token.ent_type_ == &quot;GPE&quot;] new_GPEs = [] for GPE in GPEs: GPE = GPE + &quot;_&quot; + Date GPE = GPE + &quot;_&quot; + str(article_index) new_GPEs.append(GPE) GPE_freq = Counter(new_GPEs) return(GPE_freq) #FAC - buildings, airports, brigdes etc. def FAC_freq(article): FACs = [token.text for token in article if token.ent_type_ == &quot;FAC&quot;] new_FACs = [] for FAC in FACs: FAC = FAC + &quot;_&quot; + Date FAC = FAC + &quot;_&quot; + str(article_index) new_FACs.append(FAC) FAC_freq = Counter(new_FACs) return(FAC_freq) #Person def person_freq(article): persons = [token.text for token in article if token.ent_type_ == &quot;PERSON&quot;] new_persons = [] for person in persons: person = person + &quot;_&quot; + Date person = person + &quot;_&quot; + str(article_index) new_persons.append(person) person_freq = Counter(new_persons) return(person_freq) #ORG - organisations def ORG_freq(article): ORGs = [token.text for token in article if token.ent_type_ == &quot;ORG&quot;] new_ORGs = [] for ORG in ORGs: ORG = ORG + &quot;_&quot; + Date ORG = ORG + &quot;_&quot; + str(article_index) new_ORGs.append(ORG) ORG_freq = Counter(new_ORGs) return(ORG_freq) Next up, we will iterate through all the articles and apply the helper functions to extract entities. This is also quite a large chunk but again there is a lot of repetition. First, we define an index to loop over and then we define empty counter-objects that we can append to in the loop. Then inside the loop we access the date and index of the article and the article itself. We then apply the helper functions from above to such the article dry of entities. #defining index to loop over article_index = 0 #Setting up empty Counter-objects to append to in the for-loop all_nouns = Counter() all_verbs = Counter() all_adjectives = Counter() all_GPEs = Counter() all_FACs = Counter() all_persons = Counter() all_ORGs = Counter() #looping over bread texts from all articles and running the functions on them for article in df.index: #accesing the Date of the article Date = df.iloc[article_index, 3] #load article using index article = df.iloc[article_index,5] #making article to an nlp-object article = nlp(article) #------------------------------------------------------------------- #------ Counting all objects-of-interest in Counter-objects -------- #------------------------------------------------------------------- #nouns some_nouns = noun_freq(article) all_nouns += some_nouns #verbs some_verbs = verb_freq(article) all_verbs += some_verbs #adjectives some_adjectives = adjective_freq(article) all_adjectives += some_adjectives #GPEs some_GPEs = GPE_freq(article) all_GPEs += some_GPEs #FACs some_FACs = FAC_freq(article) all_FACs += some_FACs #persons some_persons = person_freq(article) all_persons += some_persons #ORGs some_ORGs = ORG_freq(article) all_ORGs += some_ORGs #indexing to next article article_index += 1 #checking progress print(article_index) Now we have one counter object for each category, e.g. GPEs or persons. I realized that there were issues with some of the values. Here we make sure that all the values are in the same format of word_date_index. We basically remove values that has more than two \"_\". counter_objects = [all_nouns, all_verbs, all_adjectives, all_GPEs, all_FACs, all_persons, all_ORGs] for counter_object in counter_objects: to_be_deleted = [] #finding the malformity for ele in counter_object: if ele.count(&quot;_&quot;) != 2: print(ele) to_be_deleted.append(ele) #deleting the malformity for ele in to_be_deleted: if ele in counter_object: del counter_object[ele] Cleaning done. Now we have one counter object for each category and we want to save those to plain old dataframes. A format that we know and love. Again there is quite a bit of repetition but in short it goes down like this: First, the counter object is saved to a pandas dataframe, the columns are renamed and sorted by count so that values with highest count appears first. Then the values which has the format word_date_index are split into three columns, word, date and index. Lastly the dataframe is saved. The result is one dataframe for each category, each containing three columns word, date, index and count. #Saving all the counter-objects to individual dataframes #all_nouns df_all_nouns = pd.DataFrame.from_dict(all_nouns, orient=&#39;index&#39;).reset_index() df_all_nouns = df_all_nouns.rename(columns={&#39;index&#39;:&#39;word&#39;, 0:&#39;count&#39;}) df_all_nouns = df_all_nouns.sort_values(by=&#39;count&#39;, ascending = False) df_all_nouns[[&#39;noun&#39;, &#39;date&#39;, &#39;article_index&#39;]] = df_all_nouns.word.str.split(&quot;_&quot;,expand=True,) df_all_nouns.to_csv(&quot;data/new_york_times/data_NER/noun.csv&quot;, index = False) #all_verbs df_all_verbs = pd.DataFrame.from_dict(all_verbs, orient=&#39;index&#39;).reset_index() df_all_verbs = df_all_verbs.rename(columns={&#39;index&#39;:&#39;word&#39;, 0:&#39;count&#39;}) df_all_verbs = df_all_verbs.sort_values(by=&#39;count&#39;, ascending = False) df_all_verbs[[&#39;verb&#39;, &#39;date&#39;, &#39;article_index&#39;]] = df_all_verbs.word.str.split(&quot;_&quot;,expand=True,) df_all_verbs.to_csv(&quot;data/new_york_times/data_NER/verb.csv&quot;, index = False) #all_adjectives df_all_adjectives = pd.DataFrame.from_dict(all_adjectives, orient=&#39;index&#39;).reset_index() df_all_adjectives = df_all_adjectives.rename(columns={&#39;index&#39;:&#39;word&#39;, 0:&#39;count&#39;}) df_all_adjectives = df_all_adjectives.sort_values(by=&#39;count&#39;, ascending = False) df_all_adjectives[[&#39;adjective&#39;, &#39;date&#39;, &#39;article_index&#39;]] = df_all_adjectives.word.str.split(&quot;_&quot;,expand=True,) df_all_adjectives.to_csv(&quot;data/new_york_times/data_NER/adjective.csv&quot;, index = False) #all_GPEs df_all_GPEs = pd.DataFrame.from_dict(all_GPEs, orient=&#39;index&#39;).reset_index() df_all_GPEs = df_all_GPEs.rename(columns={&#39;index&#39;:&#39;word&#39;, 0:&#39;count&#39;}) df_all_GPEs = df_all_GPEs.sort_values(by=&#39;count&#39;, ascending = False) df_all_GPEs[[&#39;GPE&#39;, &#39;date&#39;, &#39;article_index&#39;]] = df_all_GPEs.word.str.split(&quot;_&quot;,expand=True,) df_all_GPEs.to_csv(&quot;data/new_york_times/data_NER/GPE.csv&quot;, index = False) #all_FACs df_all_FACs = pd.DataFrame.from_dict(all_FACs, orient=&#39;index&#39;).reset_index() df_all_FACs = df_all_FACs.rename(columns={&#39;index&#39;:&#39;word&#39;, 0:&#39;count&#39;}) df_all_FACs = df_all_FACs.sort_values(by=&#39;count&#39;, ascending = False) df_all_FACs[[&#39;FAC&#39;, &#39;date&#39;, &#39;article_index&#39;]] = df_all_FACs.word.str.split(&quot;_&quot;,expand=True,) df_all_FACs.to_csv(&quot;data/new_york_times/data_NER/FAC.csv&quot;, index = False) #all_persons df_all_persons = pd.DataFrame.from_dict(all_persons, orient=&#39;index&#39;).reset_index() df_all_persons = df_all_persons.rename(columns={&#39;index&#39;:&#39;word&#39;, 0:&#39;count&#39;}) df_all_persons = df_all_persons.sort_values(by=&#39;count&#39;, ascending = False) df_all_persons[[&#39;person&#39;, &#39;date&#39;, &#39;article_index&#39;]] = df_all_persons.word.str.split(&quot;_&quot;,expand=True,) df_all_persons.to_csv(&quot;data/new_york_times/data_NER/person.csv&quot;, index = False) #all_ORGs df_all_ORGs = pd.DataFrame.from_dict(all_ORGs, orient=&#39;index&#39;).reset_index() df_all_ORGs = df_all_ORGs.rename(columns={&#39;index&#39;:&#39;word&#39;, 0:&#39;count&#39;}) df_all_ORGs = df_all_ORGs.sort_values(by=&#39;count&#39;, ascending = False) df_all_ORGs[[&#39;ORG&#39;, &#39;date&#39;, &#39;article_index&#39;]] = df_all_ORGs.word.str.split(&quot;_&quot;,expand=True,) df_all_ORGs.to_csv(&quot;data/new_york_times/data_NER/ORG.csv&quot;, index = False) We have now successfully extracked the entities and can move on! "],["maps-vizualisations.html", "Chapter 9 Maps vizualisations 9.1 Preparing the data 9.2 Plotly Maps 9.3 Shiny App 9.4 Popcircle", " Chapter 9 Maps vizualisations div.blue { background-color:#76a5af; border-radius: 5px; padding: 20px;} All the code in the chapter is run on both the GPE dataset from NYT and The Guardian respectively but here i only show the processing of the GPE dataset from NYT. div.red { background-color:#e06666; border-radius: 5px; padding: 20px;} I only plot when the name of a country was mentioned. Mentions of cities, regions etc. does not appear in the maps. dataset from NYT. div.red { background-color:#e06666; border-radius: 5px; padding: 20px;} Some countries that are plotted does not exist anymore, e.g. Soviet. I get around this by plotting these countries as other countries that exist today. Here are the countries in question and what they are plotted as: Soviet = Russia Kosovo = Serbia Yogaslavia = Serbia Tibet = China Here I will make a map of the world showing how different parts of the world were engaged in the Taliban conflict at different points in time. For this we need GPEs. The end product is going to be two vizualisations, basically showing the same trends but in different formats: A plotly interactive map where the user can also browse through time periods. A shiny app where the user can browse through time periods. 9.1 Preparing the data We start by loading a ton of packages: tidyverse (Wickham et al. 2019), plotly (Sievert et al. 2021), tigris (Walker 2021), sf (Pebesma 2021), RColorBrewer (Neuwirth 2014), shiny (Chang et al. 2021), leaflet (Cheng, Karambelkar, and Xie 2021), htmlwidgets (Vaidyanathan et al. 2020) and popcircle (Giraud 2021). pacman::p_load(tidyverse, plotly, tigris, sf, RColorBrewer, shiny, leaflet, htmlwidgets, popcircle) Alas, we thought that the cleaning days were over, but we still need to do a bit of cleaning before we can get to the plots. We start by loading the GPE. gpe &lt;- read_csv(&quot;data/new_york_times/data_NER/GPE.csv&quot;) %&gt;% select(-word) colnames(gpe) ## [1] &quot;count&quot; &quot;GPE&quot; &quot;date&quot; &quot;article_index&quot; It has four columns: count which is the number of article hits. GPE which is the name of the GPE that has been extracted. date which is the date of the article. article_index which is the index of the article in the dataframe df_full. Then we load what is called a shapefile. I found this data on the following GitHub: https://github.com/RandomEtc/shapefile-js. shapefile &lt;- read_sf(&quot;data/data_geo/TM_WORLD_BORDERS_SIMPL-0.3.shp&quot;) colnames(shapefile) ## [1] &quot;FIPS&quot; &quot;ISO2&quot; &quot;ISO3&quot; &quot;UN&quot; &quot;NAME&quot; &quot;AREA&quot; &quot;POP2005&quot; &quot;REGION&quot; &quot;SUBREGION&quot; &quot;LON&quot; &quot;LAT&quot; ## [12] &quot;geometry&quot; Importantly for us, this file contains the three columns: NAME which is the name of the country. ISO3 which is the ISO3 code of the country, e.g. AUS for Australia. geometry which is polygon-data on the whereabouts of the country. 9.1.1 Renaming Countries Now to fixing the first problem. When merging together gpe and shapefile there are some countries that have different names in the two datasets. For example in gpe there are several names for United Kingdom such as Britain or England. We need to convert these different names for a country to a single name that fits with the country names in shapefile. This unfortunately takes some manual labor, as I dont know which different versions of country names that the NER has picked up. Here we print all the unique GPEs that appear in gpe but doesnt appear in shapefile. In other words we print those names that need to be changed. I look manually through the list to pick up countries (i.e. not cities, regions, etc.) that need to be changed. Note that nothing is printed here as it will take up too much space. I stopped looking when the count of a GPE went below 30. gpe1 &lt;- gpe %&gt;% group_by(GPE) %&gt;% summarise(count=sum(count)) %&gt;% filter(!(unique(gpe$GPE) %in% shapefile$NAME)) All of those countries that i picked out above I write here and correct them to their proper name so they fit with shapefile. I save a new file with the fixed names. #replacing names gpe &lt;- gpe %&gt;% mutate( GPE = ifelse(GPE == &quot;America&quot;, &quot;United States&quot;, GPE), GPE = ifelse(GPE == &quot;U.S.A.&quot;, &quot;United States&quot;, GPE), GPE = ifelse(GPE == &quot;USA&quot;, &quot;United States&quot;, GPE), GPE = ifelse(GPE == &quot;U.S.&quot;, &quot;United States&quot;, GPE), GPE = ifelse(GPE == &quot;States&quot;, &quot;United States&quot;, GPE), GPE = ifelse(GPE == &quot;Britain&quot;, &quot;United Kingdom&quot;, GPE), GPE = ifelse(GPE == &quot;England&quot;, &quot;United Kingdom&quot;, GPE), GPE = ifelse(GPE == &quot;Iran&quot;, &quot;Iran (Islamic Republic of)&quot;, GPE), GPE = ifelse(GPE == &quot;Korea&quot;, &quot;Korea, Democratic People&#39;s Republic of&quot;, GPE), GPE = ifelse(GPE == &quot;Libya&quot;, &quot;Libyan Arab Jamahiriya&quot;, GPE), GPE = ifelse(GPE == &quot;Syria&quot;, &quot;Syrian Arab Republic&quot;, GPE), GPE = ifelse(GPE == &quot;U.K.&quot;, &quot;United Kingdom&quot;, GPE), GPE = ifelse(GPE == &quot;UK&quot;, &quot;United Kingdom&quot;, GPE), GPE = ifelse(GPE == &quot;Czech&quot;, &quot;Czech Republic&quot;, GPE), GPE = ifelse(GPE == &quot;Culumbia&quot;, &quot;Colombia&quot;, GPE), GPE = ifelse(GPE == &quot;Soviet&quot;, &quot;Russia&quot;, GPE), GPE = ifelse(GPE == &quot;Saudi&quot;, &quot;Saudi Arabia&quot;, GPE), GPE = ifelse(GPE == &quot;Vietnam&quot;, &quot;Viet Nam&quot;, GPE), GPE = ifelse(GPE == &quot;Kosovo&quot;, &quot;Serbia&quot;, GPE), GPE = ifelse(GPE == &quot;Yugoslavia&quot;, &quot;Serbia&quot;, GPE), GPE = ifelse(GPE == &quot;Bosnia&quot;, &quot;Bosnia and Herzegovina&quot;, GPE), GPE = ifelse(GPE == &quot;Tibet&quot;, &quot;China&quot;, GPE), GPE = ifelse(GPE == &quot;ISRAEL&quot;, &quot;Israel&quot;, GPE), GPE = ifelse(GPE == &quot;Holland&quot;, &quot;Netherlands&quot;, GPE), GPE = ifelse(GPE == &quot;Deutschland&quot;, &quot;Germany&quot;, GPE), GPE = ifelse(GPE == &quot;TURKEY&quot;, &quot;Turkey&quot;, GPE), GPE = ifelse(GPE == &quot;turkey&quot;, &quot;Turkey&quot;, GPE), GPE = ifelse(GPE == &quot;pakistan&quot;, &quot;Pakistan&quot;, GPE), GPE = ifelse(GPE == &quot;SPAIN&quot;, &quot;Spain&quot;, GPE), GPE = ifelse(GPE == &quot;JAPAN&quot;, &quot;Japan&quot;, GPE), GPE = ifelse(GPE == &quot;Gaza&quot;, &quot;Palestine&quot;, GPE), GPE = ifelse(GPE == &quot;CANADA&quot;, &quot;Canada&quot;, GPE), GPE = ifelse(GPE == &quot;States&quot;, &quot;United States&quot;, GPE), GPE = ifelse(GPE == &quot;Kingdom&quot;, &quot;United Kingdom&quot;, GPE), GPE = ifelse(GPE == &quot;Zealand&quot;, &quot;New Zealand&quot;, GPE), GPE = ifelse(GPE == &quot;Lanka&quot;, &quot;Sri Lanka&quot;, GPE) ) Thus we have prepared the dataset gpe to be merged with shapefile at a later stage. 9.1.2 Every country gets a row for every date So far so good. Next up, there is a problem with the data that makes the visualizations dull. If a country is not mentioned on a specific date there is no row to signify that. There is just a missing row. Here I add a row for each country at every date where it is not mentioned and set the number of times mentioned to 0. This way every country gets a row for every date. I start by tidying up gpe: transforming GPE to factor, adding a column indicating the year, dropping the columns date and article_index, grouping by GPE and year, summarizing the sum of hits and finally filtering only the countries that also appear in shapefile. gpe &lt;- gpe %&gt;% mutate(GPE = as.factor(GPE), year = str_sub(date, 1, 4), year = as.integer(year) ) %&gt;% select(-c(date, article_index)) %&gt;% group_by(GPE, year) %&gt;% summarise(count = sum(count)) %&gt;% filter(GPE %in% shapefile$NAME) Then i find out how many unique countries that appear both in gpe and in shapefile. gpe2 &lt;- gpe %&gt;% group_by(GPE) %&gt;% summarise(count=sum(count)) %&gt;% filter(GPE %in% shapefile$NAME) Then i make two lists of unique countries and years all_countries and all_years. I then use the function expand.grid to make a new dataframe with all the possible combinations of country and years. In other words every country has a row for every year all_countries &lt;- as.list(gpe2$GPE) all_years &lt;- as.list(unique(gpe$year)) all_combs &lt;- expand.grid(all_countries, all_years) Next up, I tidy up: renaming columns, making columns to factor, adding a new column count which is equal to 0, and select the columns needed. all_combs &lt;- as_tibble(all_combs) %&gt;% rename(GPE = Var1, year = Var2) %&gt;% mutate( count = 0, GPE = as.factor(as.character(GPE)), year = as.integer(as.character(year)) ) %&gt;% select(GPE, year, count) %&gt;% arrange(year) Now i can concatenate the two dataframes gpe and all_combs in a new dataframe called gpe_large. This dataframe has 6833 observations, but we want it to only have 4131 rows, i.e. a row for each combination of country and date. So I remove all rows where values in GPE and year are the same. This way i only add a row for combinations of GPE and year that doesnt already exist. gpe_large &lt;- rbind(gpe, all_combs) gpe_large &lt;- as_tibble(gpe_large) %&gt;% distinct(GPE, year, .keep_all = TRUE) Voila. 9.1.3 Making a unique dataset for Plotly and Shiny respectively Now we can finally merge gpe and shapefile so that each country gets assigned an ISO3 code and geometry data. We use the function geo_join because shapefile is an object called sf. class(shapefile) ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; We start by making gpe_shiny which is the dataset used to make the shiny app. gpe_shiny &lt;- geo_join(shapefile, gpe_large, &quot;NAME&quot;, &quot;GPE&quot;, how = &quot;inner&quot;) %&gt;% select( c(&quot;ISO3&quot;, &quot;NAME&quot;, &quot;count&quot;, &quot;year&quot;) ) %&gt;% rename(country = NAME) colnames(gpe_shiny) ## [1] &quot;ISO3&quot; &quot;country&quot; &quot;count&quot; &quot;year&quot; &quot;geometry&quot; Alright, gpe_shiny looks ready to rock and roll. I make another dataframe called gpe_plotly which we will use to make the plotly map. For the plotly map we dont need the column geometry, we only need the ISO3-codes from the shapefile. I make this dataset by selecting the columns ISO3, country, count and year. #making new dataset gpe_plotly &lt;- tibble( ISO3 = gpe_shiny$ISO3, country = gpe_shiny$country, count = gpe_shiny$count, year = gpe_shiny$year ) 9.1.4 Making a penalized count Okay, so we dont want the plots to be too much influenced by the number of articles that were published in the given year. Therefore we will make a new variable called penalized_count which is the number of hits per article in the given year. I start by loading the good old full dataframe asdf. df1 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) df3 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;) df &lt;- rbind(df1, df2, df3) df &lt;- as_tibble(df) Then i add a new variable called year, group articles by year, and count the number of articles per year. df &lt;- df %&gt;% mutate( year = str_sub(date, 1, 4), year = as.integer(year) ) %&gt;% group_by(year) %&gt;% tally() %&gt;% rename(sum_articles_yearly = n) Then i merge df with gpe_plotly, so that we now have a column in gpe_plotly called sum_articles_yearly which indicates the total number of articles published in the given year. gpe_plotly &lt;- left_join(gpe_plotly, df, by = &quot;year&quot;) Then i make a new variable where i divide the number of hits by the total number of articles published in the given year. gpe_plotly &lt;- gpe_plotly %&gt;% mutate( penalized_count = count/sum_articles_yearly, penalized_count_round = round(penalized_count, 1) ) I apply the same steps to gpe_shiny. gpe_shiny &lt;- left_join(gpe_shiny, df, by = &quot;year&quot;) gpe_shiny &lt;- gpe_shiny %&gt;% mutate( penalized_count = count/sum_articles_yearly, penalized_count_round = round(penalized_count, 1) ) %&gt;% arrange(year) No more changes need to be added to gpe_shiny. So here i save it. saveRDS(gpe_shiny, file = &quot;data/new_york_times/gpe_shiny_NYT.rds&quot;) 9.1.5 Setting up the binsize Next step is setting up an appropiate binsize for the values in penalized_count. I make a density plot to get an overview of the data shown in figure 9.1. gpe_plotly %&gt;% ggplot() + aes(x=penalized_count) + geom_density(fill = &quot;lightblue&quot;) + theme_minimal() Figure 9.1: Density plot of penalized count. In figure 9.1 we see that an overwhelming amount of the values of penalized_count are found in in range ~0-0.1. In fact 89% of all the values of panelized_count are found in the range 0-0.1. sum(gpe_plotly$penalized_count &lt; 0.2) / length(gpe_plotly$penalized_count) ## [1] 0.9046915 Here i make another density plot showing the values in range 0-0.1. gpe_plotly %&gt;% ggplot() + aes(x=penalized_count) + geom_density(fill = &quot;lightblue&quot;) + xlim(0,0.1) + theme_minimal() Figure 9.2: Density plot of penalized count. In figure 9.2 we even see that most values are found in the range 0-0.05. We want to be able to distinguish these smaller values. That is why we change the binsize. Okay, lets get to defining the binsize. I can maximally define 9 bins. Based on 9.2 i choose to make small bins at low values and then gradually increase the size of the bins. This way smaller values are more easily distinguished. The downturn is that larger values are harder to distinguish, but remember that we have very few of these large values. mybins &lt;- c(0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, max(gpe_plotly$penalized_count)) mypalette &lt;- colorBin(palette=&quot;YlOrRd&quot;, domain=gpe_plotly$penalized_count, bins=mybins) 9.1.6 Fixing the legend scales to absolute values. The next issue with the vizualisations in Plotly (not in shiny) is that the legends are scaled to the specific year. This makes the colors on the map relative to the year and not related to the other years which is confusing and misleading. We are gonna fix this by doing something a little dirty. For each year we will add new row with the country Liechtenstein and a count of the maximum number of times a country is mentioned. Thus each year will contain a high value of count attached to the country Liechenstein. This will make the scales related to all years which is what i call absolute values. I admit it is a dirty way of fixing the legend/coloring issues. But Liechenstein is actually not present in the map so it doesnt make a difference to the final product. Here i make a new dataframe LIE with the same columns as gpe_plotly and 27 rows all containing the country Liechenstein and the maximum number in count. #making a variable with the number of years num_years &lt;- (unique(gpe_plotly$year)) #making a new dataset LIE &lt;- tibble( ISO3 = rep(&quot;LIE&quot;, length(num_years)), country = rep(&quot;Liechtenstein&quot;, length(num_years)), count = rep(max(gpe_plotly$count), length(num_years)), year = num_years, penalized_count = rep(max(gpe_plotly$penalized_count), length(num_years)), sum_articles_yearly = rep(max(gpe_plotly$sum_articles_yearly), length(num_years)), penalized_count_round = round(penalized_count, 1) ) Then i concatenate gpe_plotly and LIE. gpe_plotly &lt;- rbind(LIE, gpe_plotly) We also add a new column with a hover-text used for the map. gpe_plotly &lt;- gpe_plotly %&gt;% mutate( hover = paste0(country, &quot;\\nNumber of hits: &quot;, count, &quot;\\nNumber of articles: &quot;, sum_articles_yearly, &quot;\\nNumber of hits per article: &quot;, penalized_count_round) ) And we save it. write_csv(gpe_plotly, &quot;data/new_york_times/GPE_plotly_NYT.csv&quot;) Allright, now our data is in the right format and we can continue to the fun stuff. 9.1.7 Making a dataframe with contrasts Allright, in the shiny app we also want to show the difference in the number of hits pr. article (penalized_count) for a given country between New York Times and The Guardian. I call this difference a contrast. This way you can more easily get a grasp of the differences between the newspapers. I will one contrast: Contrast: NYT - The Guardian. So if NYT has more hits pr. article the value will be positive and if the guardian has more hits pr. article then the value will be negative. We start by loading the data for both datasets. gpe_shiny_nyt &lt;- readRDS(file = &quot;data/new_york_times/gpe_shiny_NYT.rds&quot;) gpe_shiny_guardian &lt;- readRDS(file = &quot;data/guardian/gpe_shiny_guardian.rds&quot;) Then we join the two dataframes together. Notice that we convert the sf-objects into ordinary dataframes. gpe_shiny_contrast &lt;- inner_join(gpe_shiny_nyt %&gt;% as.data.frame(), gpe_shiny_guardian %&gt;% as.data.frame(), by = c(&quot;ISO3&quot;, &quot;year&quot;, &quot;country&quot;), suffix = c(&quot;_NYT&quot;, &quot;_guardian&quot;)) Then we make two new columns for contrast_1 and contrast_2 respectively, select the columns we are interrested in and renaming the column geometry. Data wrangling bby. gpe_shiny_contrast &lt;- gpe_shiny_contrast %&gt;% mutate( contrast = (penalized_count_NYT - penalized_count_guardian), contrast_round = round(contrast, 2) ) %&gt;% select(c(ISO3, country, year, geometry_NYT, contrast, contrast_round)) %&gt;% rename(geometry = geometry_NYT) We gpe_shiny_contrast into an sf-object again and save it. gpe_shiny_contrast &lt;- st_sf(x = gpe_shiny_contrast, sf_column_name = &#39;geometry&#39;) saveRDS(gpe_shiny_contrast, file = &quot;data/new_york_times/GPE_shiny_contrast.rds&quot;) Lastly we check what the binsize should be using the same procedure as 9.1.5. I make a density plot to get an overview of the data shown in figure 9.3. gpe_shiny_contrast %&gt;% ggplot() + aes(x=contrast) + geom_density(fill = &quot;lightblue&quot;) + labs(x = &quot;contrast&quot;) + xlim(-0.25,0.25) + theme_minimal() Figure 9.3: Density plot of penalized count. In figure 9.3 we see that an overwhelming amount of the values in contrast are found in in range ~-0.05:0.05. Okay, lets get to defining the binsize. I can maximally define 11 bins. Based on 9.3 i choose to make small bins at certain values close to 0. This way some values are more easily distinguished. The downturn is that other values are harder to distinguish, but remember that we will have few of these values. mybins &lt;- c(min(gpe_shiny_contrast$contrast), -0.05, -0.0375, -0.025, -0.0125, 0, 0.0125, 0.025, 0.0375, 0.05, max(gpe_shiny_contrast$contrast)) mypalette &lt;- colorBin(palette=&quot;PRGn&quot;, domain=gpe_shiny_contrast$contrast, bins=mybins) 9.2 Plotly Maps We start by loading the datasets gpe_plotly_NYT and gpe_plotly_guardian. gpe_plotly_NYT &lt;- read_csv(&quot;data/new_york_times/GPE_plotly_NYT.csv&quot;) gpe_plotly_guardian &lt;- read_csv(&quot;data/guardian/GPE_plotly_guardian.csv&quot;) First we define some fonts and labels for the maps. This is purely for aesthetics. font = list( family = &quot;DM Sans&quot;, size = 15, color = &quot;black&quot; ) label = list( bgcolor = &quot;#EEEEEE&quot;, bordercolor = &quot;transparent&quot;, font = font ) Then we make two maps called NYT_map and guardian_map. Finally the work starts to pay off. #making the plot for NYT NYT_map &lt;- plot_geo(gpe_plotly_NYT, locationmode = &quot;world&quot;, frame = ~year) %&gt;% add_trace(locations = ~ISO3, z = ~penalized_count, color = ~penalized_count, colors = mypalette, text = ~hover, hoverinfo = &quot;text&quot;) %&gt;% layout(font = list(family = &quot;DM Sans&quot;), title = &quot;The worlds involvement in the Taliban-conflict: New York Times&quot;, legend) %&gt;% style(hoverlabel = label) %&gt;% config(displayModeBar = F) %&gt;% colorbar(title = &quot;Number of hits per article in the given year&quot;, len=0.8) #making the plot for The Guardian guardian_map &lt;- plot_geo(gpe_plotly_guardian, locationmode = &quot;world&quot;, frame = ~year) %&gt;% add_trace(locations = ~ISO3, z = ~penalized_count, color = ~penalized_count, colors = mypalette, text = ~hover, hoverinfo = &quot;text&quot;) %&gt;% layout(font = list(family = &quot;DM Sans&quot;), title = &quot;The worlds involvement in the Taliban-conflict: The Guardian&quot;, legend) %&gt;% style(hoverlabel = label) %&gt;% config(displayModeBar = F) %&gt;% colorbar(title = &quot;Number of hits per article in the given year&quot;, len=0.8) NYT_map guardian_map 9.3 Shiny App Note: Here i only show the code for the ShinyApp; it wont be able to run in a normal R-markdown. So if you want to run it you need to open up the R file called shinyapp.R in the subfolder shinyapp/. This is a special type of file used for shiny apps. The final application can be found in the folder results/. I wont go too much into the nitty-gritty of making the App, but just explain the general idea. A shinyapp consists of a front-end and a back-end. The front-end is the user-interface (ui) that the user navigates with. The back-end or the server is what happens behind the scenes to make it all work. So we define a ui and a server and then we run the application using the function shinyApp. #Loading data df_NYT &lt;- readRDS(file = &quot;gpe_shiny_NYT.rds&quot;) df_guardian &lt;- readRDS(file = &quot;GPE_shiny_guardian.rds&quot;) df_contrast &lt;- readRDS(&quot;GPE_shiny_contrast.rds&quot;) # Define UI for application ui &lt;- fluidPage( #Application title titlePanel(&quot;The world&#39;s involvement in the Taliban-conflict&quot;), #Sidebar with a date output sidebarLayout( sidebarPanel( tags$a(href=&quot;https://github.com/ah140797/taliban_newspapers&quot;, &quot;Data Repository&quot;, target = &quot;_blank&quot;), h5(&quot;These maps illustrate the worlds involvement in the Taliban-conflict using New York Times and The Guardian as sources. The colors of the countries indicate the number of hits pr article in the year selected for the given country&quot;), h5(&quot;You can switch between the tabs to select either New York Times, The Guardian and the Contrast&quot;), h5(&quot;The panel Contrast shows the difference in the number of hits pr. article for a given country between New York Times and The Guardian. Positive values (green) indicate that the country has more hits pr. article in New York Times as compared to The Guardian and vice versa.&quot;), h5(&quot;Note that mentions of cities, regions etc. does not appear in the maps&quot;), sliderInput(inputId = &quot;date&quot;, label = &quot;Select a year&quot;, min = 1996, max = 2021, value = 1996, step = 1 ) ), mainPanel( tabsetPanel( tabPanel(&quot;New York Times&quot;, leafletOutput(&quot;NYT&quot;)), tabPanel(&quot;The Guardian&quot;, leafletOutput(&quot;guardian&quot;)), tabPanel(&quot;Contrast&quot;, leafletOutput(&quot;contrast&quot;)) ) ) ) ) #define server logic server &lt;- function(input, output) { #------------------------New York Times------------------------------------- year_NYT &lt;- reactive({ w &lt;- df_NYT %&gt;% filter(year == input$date) return(w) }) output$NYT &lt;- renderLeaflet({ # Create a color palette with handmade bins. NB. play around with bin numbers mybins &lt;- c(0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, max(df_NYT$penalized_count)) mypalette &lt;- colorBin(palette=&quot;YlOrRd&quot;, domain=df_NYT$penalized_count, bins=mybins) # Prepare the text for tooltips: mytext &lt;- paste( &quot;Country: &quot;, year_NYT()$country,&quot;&lt;br/&gt;&quot;, &quot;Number of hits per article: &quot;, year_NYT()$penalized_count_round, sep=&quot;&quot;) %&gt;% lapply(htmltools::HTML) # Final Map leaflet(year_NYT()) %&gt;% addTiles() %&gt;% setView(lat=10, lng=0 , zoom=1.5) %&gt;% addPolygons( fillColor = ~mypalette(year_NYT()$penalized_count), stroke=TRUE, fillOpacity = 0.9, color=&quot;white&quot;, weight=0.3, label = mytext, highlightOptions = highlightOptions(weight = 2, fillOpacity = 1, color = &quot;black&quot;, opacity = 1, bringToFront = TRUE), labelOptions = labelOptions(style = list(&quot;font-weight&quot; = &quot;normal&quot;, padding = &quot;3px 8px&quot;), textsize = &quot;13px&quot;, direction = &quot;auto&quot;)) %&gt;% addLegend(pal=mypalette, values=~penalized_count, opacity=0.9, title = &quot;Number of hits pr. article&quot;, position = &quot;bottomright&quot;) }) #--------------------------------The Guardian------------------------------- year_guardian &lt;- reactive({ w &lt;- df_guardian %&gt;% filter(year == input$date) return(w) }) output$guardian &lt;- renderLeaflet({ # Create a color palette with handmade bins. NB. play around with bin numbers mybins &lt;- c(0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, max(df_guardian$penalized_count)) mypalette &lt;- colorBin(palette=&quot;YlOrRd&quot;, domain=df_guardian$penalized_count, bins=mybins) # Prepare the text for tooltips: mytext &lt;- paste( &quot;Country: &quot;, year_guardian()$country,&quot;&lt;br/&gt;&quot;, &quot;Number of hits per article: &quot;, year_guardian()$penalized_count_round, sep=&quot;&quot;) %&gt;% lapply(htmltools::HTML) # Final Map leaflet(year_guardian()) %&gt;% addTiles() %&gt;% setView(lat=10, lng=0 , zoom=1.5) %&gt;% addPolygons( fillColor = ~mypalette(year_guardian()$penalized_count), stroke=TRUE, fillOpacity = 0.9, color=&quot;white&quot;, weight=0.3, label = mytext, highlightOptions = highlightOptions(weight = 2, fillOpacity = 1, color = &quot;black&quot;, opacity = 1, bringToFront = TRUE), labelOptions = labelOptions(style = list(&quot;font-weight&quot; = &quot;normal&quot;, padding = &quot;3px 8px&quot;), textsize = &quot;13px&quot;, direction = &quot;auto&quot;)) %&gt;% addLegend(pal=mypalette, values=~penalized_count, opacity=0.9, title = &quot;Number of hits pr. article&quot;, position = &quot;bottomright&quot;) }) #--------------------------------Contrast------------------------------- year_contrast &lt;- reactive({ w &lt;- df_contrast %&gt;% filter(year == input$date) return(w) }) output$contrast &lt;- renderLeaflet({ # Create a color palette with handmade bins. NB. play around with bin numbers mybins &lt;- c(min(df_contrast$contrast), -0.05, -0.0375, -0.025, -0.0125, 0, 0.0125, 0.025, 0.0375, 0.05, max(df_contrast$contrast)) mypalette &lt;- colorBin(palette=&quot;PRGn&quot;, domain=df_contrast$contrast, bins=mybins) # Prepare the text for tooltips: mytext &lt;- paste( &quot;Country: &quot;, year_contrast()$country,&quot;&lt;br/&gt;&quot;, &quot;Contrast: &quot;, year_contrast()$contrast_round, sep=&quot;&quot;) %&gt;% lapply(htmltools::HTML) # Final Map leaflet(year_contrast()) %&gt;% addTiles() %&gt;% setView(lat=10, lng=0 , zoom=1.5) %&gt;% addPolygons( fillColor = ~mypalette(year_contrast()$contrast), stroke=TRUE, fillOpacity = 0.9, color=&quot;white&quot;, weight=0.3, label = mytext, highlightOptions = highlightOptions(weight = 2, fillOpacity = 1, color = &quot;black&quot;, opacity = 1, bringToFront = TRUE), labelOptions = labelOptions(style = list(&quot;font-weight&quot; = &quot;normal&quot;, padding = &quot;3px 8px&quot;), textsize = &quot;13px&quot;, direction = &quot;auto&quot;)) %&gt;% addLegend(pal=mypalette, values=~contrast, opacity=0.9, title = &quot;Contrast&quot;, position = &quot;bottomright&quot;) }) } #run the application 9.4 Popcircle This link will give you some background information: https://github.com/rcarto/popcircle. Allright lets go. We start by making the datasets we need. We take the dataset gpe_plotly, group it by country, summarize the sum of count and making a new column count_per_article which indicates how many times a country appears pr. article. gpe_popcircle_NYT &lt;- gpe_plotly %&gt;% group_by(country, ISO3) %&gt;% summarize(count = sum(count), count_per_article = count/21109) Then we merge gpe_popcircle_NYT with gpe_shiny to get the column geometry back into the mix. We make gpe_popcircle_NYT an object of class sf and finally save it as an rds-object. gpe_popcircle_NYT &lt;- left_join(gpe_popcircle_NYT, gpe_shiny, by = &quot;ISO3&quot;) %&gt;% select(c(&quot;country.x&quot;, ISO3, &quot;count.x&quot;, count_per_article, geometry)) %&gt;% distinct() gpe_popcircle_NYT &lt;- st_as_sf(gpe_popcircle_NYT) saveRDS(gpe_popcircle_NYT, file = &quot;data/new_york_times/gpe_popcircle_NYT.rds&quot;) Here we load the data. gpe_popcircle_nyt &lt;- readRDS(&quot;data/new_york_times/gpe_popcircle_NYT.rds&quot;) gpe_popcircle_guardian &lt;- readRDS(&quot;data/guardian/gpe_popcircle_guardian.rds&quot;) In the next chunk we make a new object called nyt which is used to make the plot. #there were some error and this fixed it :) sf::sf_use_s2(FALSE) # Computes circles and polygons nyt &lt;- popcircle(x = gpe_popcircle_nyt, var = &quot;count_per_article&quot;) circles_nyt &lt;- nyt$circles shapes_nyt &lt;- nyt$shapes shapes_nyt &lt;- st_transform(shapes_nyt, 4326) circles_nyt &lt;- st_transform(circles_nyt, 4326) # Create labels shapes_nyt$lab &lt;- paste0(&quot;&lt;b&gt;&quot;, shapes_nyt$country.x, &quot;&lt;/b&gt; &lt;br&gt;&quot;, round(shapes_nyt$count_per_article, 2), &quot; Hits per Article&quot;) We also make an object for The Guardian called guardian. # Computes circles and polygons guardian &lt;- popcircle(x = gpe_popcircle_guardian, var = &quot;count_per_article&quot;) circles_guardian &lt;- guardian$circles shapes_guardian &lt;- guardian$shapes shapes_guardian &lt;- st_transform(shapes_guardian, 4326) circles_guardian &lt;- st_transform(circles_guardian, 4326) # Create labels shapes_guardian$lab &lt;- paste0(&quot;&lt;b&gt;&quot;, shapes_guardian$country.x, &quot;&lt;/b&gt; &lt;br&gt;&quot;, round(shapes_guardian$count_per_article, 2), &quot; Hits per Article&quot;) Then we make the visualizations. 9.4.1 New York Times # Create the interactive visualisation leaflet(shapes_nyt, width=800, height = 750) %&gt;% addPolygons(data = circles_nyt, opacity = 1, color = &quot;white&quot;, weight = 1.5, options = list(interactive = FALSE), fill = T, fillColor = &quot;#757083&quot;, fillOpacity = 1, smoothFactor = 0) %&gt;% addPolygons(data = shapes_nyt, opacity = .8, color = &quot;#88292f&quot;, weight = 1.5, popup = shapes_nyt$lab, options = list(clickable = TRUE), fill = T, fillColor = &quot;#88292f&quot;, fillOpacity = .9, smoothFactor = .5) 9.4.2 The Guardian # Create the interactive visualisation leaflet(shapes_guardian, width=800, height = 750) %&gt;% addPolygons(data = circles_guardian, opacity = 1, color = &quot;white&quot;, weight = 1.5, options = list(interactive = FALSE), fill = T, fillColor = &quot;#757083&quot;, fillOpacity = 1, smoothFactor = 0) %&gt;% addPolygons(data = shapes_guardian, opacity = .8, color = &quot;#88292f&quot;, weight = 1.5,popup = shapes_guardian$lab, options = list(clickable = TRUE), fill = T, fillColor = &quot;#88292f&quot;, fillOpacity = .9, smoothFactor = .5) References "],["persons-vizualisation.html", "Chapter 10 Persons Vizualisation 10.1 Standardize to hits pr. article and grouping by month 10.2 Color Palettes 10.3 United States Presidents 10.4 British Prime Ministers 10.5 Afghanistan Presidents 10.6 Other Persons", " Chapter 10 Persons Vizualisation In this chapter I will make visualizations of key persons to see in which periods of time that these key persons were relevant for the Taliban-conflict. I also compare the two newspapers to see if they focus more or less on these key persons. The code in this chapter is mainly for making plots. The commenting will be sparse as it would be too tidious to comment everything. First thing first, we start by loading the package tidyverse (Wickham et al. 2019), patchwork (Pedersen 2020) and wesanderson (Ram and Wickham 2018). pacman::p_load(tidyverse, patchwork, wesanderson) Then we load the data containing persons. person_nyt &lt;- read_csv(&quot;data/new_york_times/data_NER/person.csv&quot;) person_guardian &lt;- read_csv(&quot;data/guardian/data_NER/person.csv&quot;) We do a quick inspection of the most important persons. z &lt;- person_nyt %&gt;% group_by(person) %&gt;% summarize(n = sum(count)) %&gt;% arrange(desc(n)) g &lt;- person_guardian %&gt;% group_by(person) %&gt;% summarize(n = sum(count)) %&gt;% arrange(desc(n)) The following key persons will be plotted: United States presidents George W. Bush Barack Obama Donald Trump British Prime Minister Tony Blair Gordon Brown David Cameron Boris Johnson Afghanistan presidents Hamid Karzai Ashraf Ghani Other Persons Osama bin Laden Saddan Hussein div.red { background-color:#e06666; border-radius: 5px; padding: 20px;} Note that Theresa May, British prime minister between 2016-06-13 and 2019-07-24, was not picked up by spaCy and she will therefore not be plotted. 10.1 Standardize to hits pr. article and grouping by month In order to compare the two newspapers we need to standardize the count according to the number of articles pr. month. We will make a new variable called penalized_count for both person_nyt and person_guardian. We start by adding a new column month to person_nyt and person_guardian. person_nyt &lt;- person_nyt %&gt;% mutate(month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) person_guardian &lt;- person_guardian %&gt;% mutate(month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) Then we load the dataset containing articles for both newspapers as df_nyt and df_guardian. df1 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) df3 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;) df &lt;- rbind(df1, df2, df3) df_nyt &lt;- as_tibble(df) df_guardian &lt;- read_csv(&quot;data/guardian/guardian_clean.csv&quot;) Here we add a new column to each dataset called month . df_nyt &lt;- df_nyt %&gt;% mutate(month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) df_guardian &lt;- df_guardian %&gt;% mutate(month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) Then we add a new column sum_articles_month which is the sum of articles for a given month. df_nyt &lt;- df_nyt %&gt;% group_by(month) %&gt;% tally() %&gt;% rename(sum_articles_month = n) df_guardian &lt;- df_guardian %&gt;% group_by(month) %&gt;% tally() %&gt;% rename(sum_articles_month = n) For the person datasets, we group by person and month and summarize the count. So we get a score for each person for each month. person_nyt &lt;- person_nyt %&gt;% group_by(person, month) %&gt;% summarise(count = sum(count)) person_guardian &lt;- person_guardian %&gt;% group_by(person, month) %&gt;% summarise(count = sum(count)) Then we merge df_nyt with person_nyt so that we now have a column in person_nyt called sum_articles_month which indicates the total number of articles published in the given month. I do the same for df_guardian and person_guardian. person_nyt &lt;- left_join(person_nyt, df_nyt, by = &quot;month&quot;) person_guardian &lt;- left_join(person_guardian, df_guardian, by = &quot;month&quot;) Then we make the columns penalized_count and penalized_count_round which indicates the number of hits for a given person pr. month pr. total articles in that month.We also make a column newspaper to indicate the newspaper. Some months have very few articles, so we also filter out out these, because they can make unreliable values of penalized_count. person_nyt &lt;- person_nyt %&gt;% mutate( penalized_count = count/sum_articles_month, penalized_count_round = round(penalized_count, 1), newspaper = &quot;New York Times&quot; ) %&gt;% filter(sum_articles_month &gt; 15) person_guardian &lt;- person_guardian %&gt;% mutate( penalized_count = count/sum_articles_month, penalized_count_round = round(penalized_count, 1), newspaper = &quot;The Guardian&quot; ) %&gt;% filter(sum_articles_month &gt; 15) Then i bind the datasets together so we can plot them in the same plot and select the columns needed. df_person &lt;- rbind(person_nyt, person_guardian) df_person &lt;- as_tibble(df_person) Now we have a dataset that is ready for plots. 10.2 Color Palettes Here I quickly define some color palettes with colors that i like. These palettes will be used for different plots. The same colors will be used consistently throughout the notebook. color_palette_newspaper &lt;- c(wes_palette(&quot;Chevalier1&quot;)[1], wes_palette(&quot;Darjeeling2&quot;)[2]) color_palette_afghan_pres &lt;- c(wes_palette(&quot;Moonrise1&quot;)[2], wes_palette(&quot;Moonrise3&quot;)[3]) color_palette_terrorists &lt;- c(wes_palette(&quot;Royal1&quot;)[4], wes_palette(&quot;Royal2&quot;)[5]) 10.3 United States Presidents 10.3.1 Summary plot summary_nyt &lt;- df_person %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% filter(person %in% c(&quot;Bush&quot;, &quot;Obama&quot;, &quot;Trump&quot;)) %&gt;% group_by(person) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/21109) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(person, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[1]) + coord_flip() + labs(x= &quot;President&quot;, y=&quot;Hits per Article&quot;, title = &quot;New York Times&quot;) + theme_minimal() + ylim(0,1) summary_guardian &lt;- df_person %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% filter(person %in% c(&quot;Bush&quot;, &quot;Obama&quot;, &quot;Trump&quot;)) %&gt;% group_by(person) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/9609) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(person, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[2]) + coord_flip() + labs(x= &quot;President&quot;, y=&quot;Hits per Article&quot;, title = &quot;The Guardian&quot;) + theme_minimal() + ylim(0,1) u &lt;- summary_nyt / summary_guardian + plot_annotation(title = &quot;U.S. Presidents&quot;, theme = theme(plot.title = element_text(size=18))) 10.3.2 George W. Bush bush &lt;- df_person %&gt;% filter(person == &quot;Bush&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;George W. Bush&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2001-01-20&quot;), as.Date(&quot;2009-01-20&quot;))), linetype = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;solid&quot;, &quot;longdash&quot;)) bush 10.3.3 Barack Obama obama &lt;- df_person %&gt;% filter(person == &quot;Obama&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Barack Obama&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;2004-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2009-01-20&quot;), as.Date(&quot;2017-01-20&quot;))), linetype = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;solid&quot;, &quot;longdash&quot;)) obama 10.3.4 Donald Trump trump &lt;- df_person %&gt;% filter(person == &quot;Trump&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Donald Trump&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;2004-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2017-01-20&quot;), as.Date(&quot;2021-01-20&quot;))), linetype = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;solid&quot;, &quot;longdash&quot;)) trump 10.3.5 Combined Plots 10.3.5.1 New York Times United states president plot for New York Times bush &lt;- df_person %&gt;% filter(person == &quot;Bush&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( president = &quot;George W. Bush&quot; ) obama &lt;- df_person %&gt;% filter(person == &quot;Obama&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( president = &quot;Barack Obama&quot; ) trump &lt;- df_person %&gt;% filter(person == &quot;Trump&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( president = &quot;Donald Trump&quot; ) pres &lt;- rbind(bush, obama, trump) pres &lt;- as_tibble(pres) %&gt;% mutate(president = as.factor(president)) pres &lt;- pres %&gt;% mutate(president = fct_relevel(president, c(&quot;George W. Bush&quot;, &quot;Barack Obama&quot;))) pres_nyt &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = president) + geom_point(size = 0.9) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;U.S Presidents Mentioned in New York Times&quot;) + scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = wes_palette(name = &quot;FantasticFox1&quot;), name = &quot;&quot;) pres_nyt 10.3.5.2 The Guardian United States president plot for the Guardian. bush &lt;- df_person %&gt;% filter(person == &quot;Bush&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( president = &quot;George W. Bush&quot; ) obama &lt;- df_person %&gt;% filter(person == &quot;Obama&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( president = &quot;Barack Obama&quot; ) trump &lt;- df_person %&gt;% filter(person == &quot;Trump&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( president = &quot;Donald Trump&quot; ) pres &lt;- rbind(bush, obama, trump) pres &lt;- as_tibble(pres) %&gt;% mutate(president = as.factor(president)) pres &lt;- pres %&gt;% mutate(president = fct_relevel(president, c(&quot;George W. Bush&quot;, &quot;Barack Obama&quot;))) pres_guardian &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = president) + geom_point(size = 0.9) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;U.S Presidents Mentioned in The Guardian&quot;) + scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = wes_palette(name = &quot;FantasticFox1&quot;), name = &quot;&quot;) pres_guardian 10.3.5.3 Both Newspapers Without Facet Wrap pres_nyt_y &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,7) + #scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2001-01-20&quot;), as.Date(&quot;2009-01-20&quot;), as.Date(&quot;2017-01-20&quot;))), linetype = c(&quot;solid&quot;, &quot;solid&quot;, &quot;solid&quot;), color = c(wes_palette(&quot;FantasticFox1&quot;)[1], wes_palette(&quot;FantasticFox1&quot;)[2], wes_palette(&quot;FantasticFox1&quot;)[3])) pres_guardian_y &lt;- pres_guardian + theme(legend.position=&quot;bottom&quot;) + ylim(-0.5,7) + #scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2001-01-20&quot;), as.Date(&quot;2009-01-20&quot;), as.Date(&quot;2017-01-20&quot;))), linetype = c(&quot;solid&quot;, &quot;solid&quot;, &quot;solid&quot;), color = c(wes_palette(&quot;FantasticFox1&quot;)[1], wes_palette(&quot;FantasticFox1&quot;)[2], wes_palette(&quot;FantasticFox1&quot;)[3])) #combining into single plot pres_both_newspapers &lt;- pres_nyt_y / pres_guardian_y #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers 10.3.5.4 Both Newspapers With Facet Wrap pres_nyt &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,3) + #scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + facet_wrap(~president, nrow = 1) + labs(title = &quot;New York Times&quot;) pres_guardian &lt;- pres_guardian + theme(legend.position=&quot;none&quot;) + ylim(-0.5,3) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;5 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~president, nrow = 1) + labs(title = &quot;The Guardian&quot;) #combining into single plot pres_both_newspapers &lt;- pres_nyt / pres_guardian #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers + plot_annotation(title = &quot;Timeline of U.S. Presidents&quot;, theme = theme(plot.title = element_text(size=18))) 10.4 British Prime Ministers div.red { background-color:#e06666; border-radius: 5px; padding: 20px;} Note that Theresa May, British prime minister between 2016-06-13 and 2019-07-24, was not picked up by spaCy and she will therefore not be plotted. 10.4.1 Summary plot summary_nyt &lt;- df_person %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% filter(person %in% c(&quot;Blair&quot;, &quot;Brown&quot;, &quot;Cameron&quot;, &quot;Boris&quot;)) %&gt;% group_by(person) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/21109) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(person, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[1]) + coord_flip() + labs(x= &quot;Prime Minister&quot;, y=&quot;Hits per Article&quot;, title = &quot;New York Times&quot;) + theme_minimal() + ylim(0,0.35) summary_guardian &lt;- df_person %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% filter(person %in% c(&quot;Blair&quot;, &quot;Brown&quot;, &quot;Cameron&quot;, &quot;Boris&quot;)) %&gt;% group_by(person) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/9609) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(person, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[2]) + coord_flip() + labs(x= &quot;Prime Minister&quot;, y=&quot;Hits per Article&quot;, title = &quot;The Guardian&quot;) + theme_minimal() + ylim(0,0.35) b &lt;- summary_nyt / summary_guardian + plot_annotation(title = &quot;British Prime Ministers&quot;, theme = theme(plot.title = element_text(size=18))) b 10.4.2 Tony Blair blair &lt;- df_person %&gt;% filter(person == &quot;Blair&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Tony Blair&quot;) + facet_wrap(~newspaper, nrow = 2) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;1997-05-02&quot;), as.Date(&quot;2007-06-27&quot;))), linetype = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;solid&quot;, &quot;longdash&quot;)) + scale_x_date(limits = as.Date(c(&quot;1997-01-01&quot;, &quot;2021-11-01&quot;))) blair ### Gordon Brown brown &lt;- df_person %&gt;% filter(person == &quot;Brown&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Gordon Brown&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(date_breaks = &quot;3 year&quot;) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2007-06-27&quot;), as.Date(&quot;2010-05-11&quot;))), linetype = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;solid&quot;, &quot;longdash&quot;)) + scale_x_date(limits = as.Date(c(&quot;1997-01-01&quot;, &quot;2021-11-01&quot;))) brown 10.4.3 David Cameron cameron &lt;- df_person %&gt;% filter(person == &quot;Cameron&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;David Cameron&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(date_breaks = &quot;3 year&quot;) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2010-05-11&quot;), as.Date(&quot;2016-06-13&quot;))), linetype = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;solid&quot;, &quot;longdash&quot;)) + scale_x_date(limits = as.Date(c(&quot;2000-01-01&quot;, &quot;2021-11-01&quot;))) cameron 10.4.4 Boris Johnson boris &lt;- df_person %&gt;% filter(person == &quot;Boris&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Boris Johnson&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(date_breaks = &quot;3 year&quot;) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2019-06-24&quot;))), linetype = c(&quot;solid&quot;, &quot;solid&quot;)) + scale_x_date(limits = as.Date(c(&quot;1997-01-01&quot;, &quot;2021-11-01&quot;))) boris ### Combined Plots 10.4.4.1 New York Times British prime minister plot for New York Times. blair &lt;- df_person %&gt;% filter(person == &quot;Blair&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( president = &quot;Tony Blair&quot; ) brown &lt;- df_person %&gt;% filter(person == &quot;Brown&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( president = &quot;Gordon Brown&quot; ) cameron &lt;- df_person %&gt;% filter(person == &quot;Cameron&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( president = &quot;David Cameron&quot; ) boris &lt;- df_person %&gt;% filter(person == &quot;Boris&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( president = &quot;Boris Johnson&quot; ) pres &lt;- rbind(blair, brown, cameron, boris) pres &lt;- as_tibble(pres) %&gt;% mutate(president = as.factor(president)) pres &lt;- pres %&gt;% mutate(president = fct_relevel(president, c(&quot;Tony Blair&quot;, &quot;Gordon Brown&quot;, &quot;David Cameron&quot;))) pres_nyt &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = president) + geom_point(size = 0.7) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;British Prime Ministers Mentioned in New York Times&quot;) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = wes_palette(name = &quot;Darjeeling1&quot;), name = &quot;&quot;) pres_nyt 10.4.4.2 The Guardian British prime minister plot for the Guardian. blair &lt;- df_person %&gt;% filter(person == &quot;Blair&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( president = &quot;Tony Blair&quot; ) brown &lt;- df_person %&gt;% filter(person == &quot;Brown&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( president = &quot;Gordon Brown&quot; ) cameron &lt;- df_person %&gt;% filter(person == &quot;Cameron&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( president = &quot;David Cameron&quot; ) boris &lt;- df_person %&gt;% filter(person == &quot;Boris&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( president = &quot;Boris Johnson&quot; ) pres &lt;- rbind(blair, brown, cameron, boris) pres &lt;- as_tibble(pres) %&gt;% mutate(president = as.factor(president)) pres &lt;- pres %&gt;% mutate(president = fct_relevel(president, c(&quot;Tony Blair&quot;, &quot;Gordon Brown&quot;, &quot;David Cameron&quot;))) pres_guardian &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = president) + geom_point(size = 0.7) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;British Prime Ministers Mentioned in The Guardian&quot;) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = wes_palette(name = &quot;Darjeeling1&quot;), name = &quot;&quot;) pres_guardian 10.4.4.3 Both Newspapers Without Facet Wrap pres_nyt_y &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,4) + #scale_x_date(limits = as.Date(c(&quot;1997-01-01&quot;, &quot;2021-11-01&quot;))) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;1997-05-02&quot;), as.Date(&quot;2007-06-27&quot;), as.Date(&quot;2010-05-11&quot;), as.Date(&quot;2019-06-24&quot;))), linetype = c(&quot;solid&quot;, &quot;solid&quot;, &quot;solid&quot;, &quot;solid&quot;), color = c(wes_palette(&quot;Darjeeling1&quot;)[1], wes_palette(&quot;Darjeeling1&quot;)[2], wes_palette(&quot;Darjeeling1&quot;)[3], wes_palette(&quot;Darjeeling1&quot;)[4])) pres_guardian_y &lt;- pres_guardian + theme(legend.position=&quot;bottom&quot;) + ylim(-0.5,4) + #scale_x_date(limits = as.Date(c(&quot;1997-01-01&quot;, &quot;2021-11-01&quot;))) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;1997-05-02&quot;), as.Date(&quot;2007-06-27&quot;), as.Date(&quot;2010-05-11&quot;), as.Date(&quot;2019-06-24&quot;))), linetype = c(&quot;solid&quot;, &quot;solid&quot;, &quot;solid&quot;, &quot;solid&quot;), color = c(wes_palette(&quot;Darjeeling1&quot;)[1], wes_palette(&quot;Darjeeling1&quot;)[2], wes_palette(&quot;Darjeeling1&quot;)[3], wes_palette(&quot;Darjeeling1&quot;)[4])) #combining into single plot pres_both_newspapers &lt;- pres_nyt_y / pres_guardian_y #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers 10.4.4.4 Both Newspapers With Facet Wrap pres_nyt &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,4) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;6 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~president, nrow = 1) + labs(title = &quot;New York Times&quot;) pres_guardian &lt;- pres_guardian + theme(legend.position=&quot;none&quot;) + ylim(-0.5,4) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;6 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~president, nrow = 1) + labs(title = &quot;The Guardian&quot;) #combining into single plot pres_both_newspapers &lt;- pres_nyt / pres_guardian #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers + plot_annotation(title = &quot;Timeline of British Prime Ministers&quot;, theme = theme(plot.title = element_text(size=18))) 10.5 Afghanistan Presidents 10.5.1 Summary plot summary_nyt &lt;- df_person %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% filter(person %in% c(&quot;Karzai&quot;, &quot;Ghani&quot;)) %&gt;% group_by(person) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/21109) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(person, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[1]) + coord_flip() + labs(x= &quot;President&quot;, y=&quot;Hits per Article&quot;, title = &quot;New York Times&quot;) + theme_minimal() + ylim(0,0.6) summary_guardian &lt;- df_person %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% filter(person %in% c(&quot;Karzai&quot;, &quot;Ghani&quot;)) %&gt;% group_by(person) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/9609) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(person, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[2]) + coord_flip() + labs(x= &quot;President&quot;, y=&quot;Hits per Article&quot;, title = &quot;The Guardian&quot;) + theme_minimal() + ylim(0,0.6) a &lt;- summary_nyt / summary_guardian + plot_annotation(title = &quot;Afghanistan Presidents&quot;, theme = theme(plot.title = element_text(size=18))) a 10.5.2 Hamid Karzai karzai &lt;- df_person %&gt;% filter(person == &quot;Karzai&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Hamid Karzai&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;2000-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) karzai 10.5.3 Ashraf Ghani ghani &lt;- df_person %&gt;% filter(person == &quot;Ghani&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Ashraf Ghani&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) ghani 10.5.4 Combined Plots 10.5.4.1 New York Times Afghanistan president plot for New York Times. karzai &lt;- df_person %&gt;% filter(person == &quot;Karzai&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( president = &quot;Hamid Karzai&quot; ) ghani &lt;- df_person %&gt;% filter(person == &quot;Ghani&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( president = &quot;Ashraf Ghani&quot; ) pres &lt;- rbind(karzai, ghani) pres &lt;- as_tibble(pres) %&gt;% mutate(president = as.factor(president)) pres &lt;- pres %&gt;% mutate(president = fct_relevel(president, &quot;Hamid Karzai&quot;)) pres_nyt &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = president) + geom_point(size = 0.9) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Afghan Presidents Mentioned in New York Times&quot;) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_afghan_pres, name = &quot;&quot;) pres_nyt 10.5.4.2 The Guardian Afghanistan president plot for the Guardian. karzai &lt;- df_person %&gt;% filter(person == &quot;Karzai&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( president = &quot;Hamid Karzai&quot; ) ghani &lt;- df_person %&gt;% filter(person == &quot;Ghani&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( president = &quot;Ashraf Ghani&quot; ) pres &lt;- rbind(karzai, ghani) pres &lt;- as_tibble(pres) %&gt;% mutate(president = as.factor(president)) pres &lt;- pres %&gt;% mutate(president = fct_relevel(president, &quot;Hamid Karzai&quot;)) pres_guardian &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = president) + geom_point(size = 0.9) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Afghan Presidents Mentioned in The Guardian&quot;) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_afghan_pres, name = &quot;&quot;) pres_guardian 10.5.4.3 Both Newspapers Without Facet Wrap pres_nyt_t &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,3.5) + #scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2001-12-22&quot;), as.Date(&quot;2014-09-29&quot;))), linetype = c(&quot;solid&quot;, &quot;solid&quot;), color = c(color_palette_afghan_pres[1], color_palette_afghan_pres[2])) pres_guardian_t &lt;- pres_guardian + theme(legend.position=&quot;bottom&quot;) + ylim(-0.5,3.5) + #scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2001-12-22&quot;), as.Date(&quot;2014-09-29&quot;))), linetype = c(&quot;solid&quot;, &quot;solid&quot;), color = c(color_palette_afghan_pres[1], color_palette_afghan_pres[2])) #combining into single plot pres_both_newspapers &lt;- pres_nyt_t / pres_guardian_t #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers 10.5.4.4 Both Newspapers With Facet Wrap #removing legends pres_nyt &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,3) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;3 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~president, nrow = 1) + labs(title = &quot;New York Times&quot;) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. pres_guardian &lt;- pres_guardian + theme(legend.position=&quot;none&quot;) + ylim(-0.5,3) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;3 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~president, nrow = 1) + labs(title = &quot;The Guardian&quot;) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. #combining into single plot pres_both_newspapers &lt;- pres_nyt / pres_guardian #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers + plot_annotation(title = &quot;Timeline of Afghanistan Presidents&quot;, theme = theme(plot.title = element_text(size=18))) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). ## `geom_smooth()` using formula &#39;y ~ x&#39; 10.6 Other Persons 10.6.1 Summary plot summary_nyt &lt;- df_person %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% filter(person %in% c(&quot;Laden&quot;, &quot;Hussein&quot;)) %&gt;% group_by(person) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/21109) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(person, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[1]) + coord_flip() + labs(x= &quot;Terrosist&quot;, y=&quot;Hits per Article&quot;, title = &quot;New York Times&quot;) + theme_minimal() + ylim(0,0.6) summary_guardian &lt;- df_person %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% filter(person %in% c(&quot;Laden&quot;, &quot;Hussein&quot;)) %&gt;% group_by(person) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/9609) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(person, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[2]) + coord_flip() + labs(x= &quot;Terrosist&quot;, y=&quot;Hits per Article&quot;, title = &quot;The Guardian&quot;) + theme_minimal() + ylim(0,0.6) t &lt;- summary_nyt / summary_guardian + plot_annotation(title = &quot;Terrorists&quot;, theme = theme(plot.title = element_text(size=18))) t 10.6.2 Osama bin Laden laden &lt;- df_person %&gt;% filter(person == &quot;Laden&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Osama bin Laden&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) + ylim(0,8) laden ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). 10.6.3 Saddam Hussein hussein &lt;- df_person %&gt;% filter(person == &quot;Hussein&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Saddam Hussein&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) hussein 10.6.4 Combined Plots 10.6.4.1 New York Times Terrorist plot for New York Times. laden &lt;- df_person %&gt;% filter(person == &quot;Laden&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( terrorist = &quot;Osama bin Laden&quot; ) hussein &lt;- df_person %&gt;% filter(person == &quot;Hussein&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( terrorist = &quot;Saddam Hussein&quot; ) pres &lt;- rbind(laden, hussein) pres &lt;- as_tibble(pres) %&gt;% mutate(president = as.factor(terrorist)) pres_nyt &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = terrorist) + geom_point(size = 0.9) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Terrorists Mentioned in New York Times&quot;) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_terrorists, name = &quot;&quot;) pres_nyt 10.6.4.2 The Guardian Afghanistan president plot for the Guardian. laden &lt;- df_person %&gt;% filter(person == &quot;Laden&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( terrorist = &quot;Osama bin Laden&quot; ) hussein &lt;- df_person %&gt;% filter(person == &quot;Hussein&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( terrorist = &quot;Saddam Hussein&quot; ) pres &lt;- rbind(laden, hussein) pres &lt;- as_tibble(pres) %&gt;% mutate(president = as.factor(terrorist)) pres_guardian &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = terrorist) + geom_point(size = 0.9) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Terrorists Mentioned in The Guardian&quot;) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_terrorists, name = &quot;&quot;) pres_guardian 10.6.4.3 Both Newspapers Without Facet Wrap pres_nyt_t &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,3.5) + #scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2001-12-22&quot;), as.Date(&quot;2014-09-29&quot;))), linetype = c(&quot;solid&quot;, &quot;solid&quot;), color = c(color_palette_afghan_pres[1], color_palette_afghan_pres[2])) pres_guardian_t &lt;- pres_guardian + theme(legend.position=&quot;bottom&quot;) + ylim(-0.5,3.5) + #scale_x_date(limits = as.Date(c(&quot;1998-01-01&quot;, &quot;2021-11-01&quot;))) + geom_vline(xintercept = as.numeric(c(as.Date(&quot;2001-12-22&quot;), as.Date(&quot;2014-09-29&quot;))), linetype = c(&quot;solid&quot;, &quot;solid&quot;), color = c(color_palette_afghan_pres[1], color_palette_afghan_pres[2])) #combining into single plot pres_both_newspapers &lt;- pres_nyt_t / pres_guardian_t #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers ## Warning: Removed 6 rows containing non-finite values (stat_smooth). ## Warning: Removed 6 rows containing missing values (geom_point). ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). 10.6.4.4 Both Newspapers With Facet Wrap #removing legends pres_nyt &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,3) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;3 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~president, nrow = 1) + labs(title = &quot;New York Times&quot;) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. pres_guardian &lt;- pres_guardian + theme(legend.position=&quot;none&quot;) + ylim(-0.5,3) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;3 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~president, nrow = 1) + labs(title = &quot;The Guardian&quot;) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will replace the existing scale. #combining into single plot pres_both_newspapers &lt;- pres_nyt / pres_guardian #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers + plot_annotation(title = &quot;Timeline of Terrorists&quot;, theme = theme(plot.title = element_text(size=18))) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 8 rows containing non-finite values (stat_smooth). ## Warning: Removed 8 rows containing missing values (geom_point). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). References "],["organisations-vizualisations.html", "Chapter 11 Organisations Vizualisations 11.1 Standardize names of organisations 11.2 Standardize to hits pr. article and grouping by month 11.3 Color Palettes 11.4 Summary plot 11.5 NATO 11.6 Taliban 11.7 Al Qaeda 11.8 Hamas 11.9 Hezbollah 11.10 Combined Plots for all Key Organisations", " Chapter 11 Organisations Vizualisations In this Chapter I will make vizualisations of key organisations to see which periods of time that these organizations were relevant for the Taliban-conflict. I also compare the two newspapers to see if they focus more or less on these key organizations The code in this chapter is mainly for making plots. The commenting will be sparse as it would be too tedious to comment everything. First thing first, we start by loading the package tidyverse (Wickham et al. 2019), patchwork (Pedersen 2020) and wesanderson (Ram and Wickham 2018). pacman::p_load(tidyverse, patchwork, wesanderson) Then we load the dataset containing organisations for both newspapers. org_nyt &lt;- read_csv(&quot;data/new_york_times/data_NER/org.csv&quot;) org_guardian &lt;- read_csv(&quot;data/guardian/data_NER/org.csv&quot;) Here we get a quick look at what is interesting to pursue in our plots. org_nyt %&gt;% group_by(ORG) %&gt;% summarize(n = sum(count)) %&gt;% arrange(desc(n)) org_guardian %&gt;% group_by(ORG) %&gt;% summarize(n = sum(count)) %&gt;% arrange(desc(n)) The following key organizations will be plotted: Western Alliances Nato Militant Islamic Organizations Taliban Al Queda Hamas Hezbollah 11.1 Standardize names of organisations The name of organizations appears differently. For example NATO appears under several different names such as Nato or NATO. We will standardize these names so they all appear as the same name. org_nyt &lt;- org_nyt %&gt;% mutate( #NATO ORG = ifelse(ORG == &quot;nato&quot;, &quot;NATO&quot;, ORG), ORG = ifelse(ORG == &quot;Nato&quot;, &quot;NATO&quot;, ORG), #Al Qaeda ORG = ifelse(ORG == &quot;qaeda&quot;, &quot;Qaeda&quot;, ORG), ORG = ifelse(ORG == &quot;Qaida&quot;, &quot;Qaeda&quot;, ORG), #Taliban ORG = ifelse(ORG == &quot;taliban&quot;, &quot;Taliban&quot;, ORG), #Hizbollah ORG = ifelse(ORG == &quot;Hizbollah&quot;, &quot;Hezbollah&quot;, ORG), ) org_guardian &lt;- org_guardian %&gt;% mutate( #NATO ORG = ifelse(ORG == &quot;nato&quot;, &quot;NATO&quot;, ORG), ORG = ifelse(ORG == &quot;Nato&quot;, &quot;NATO&quot;, ORG), #Al Qaeda ORG = ifelse(ORG == &quot;qaeda&quot;, &quot;Qaeda&quot;, ORG), ORG = ifelse(ORG == &quot;Qaida&quot;, &quot;Qaeda&quot;, ORG), #Taliban ORG = ifelse(ORG == &quot;taliban&quot;, &quot;Taliban&quot;, ORG), #Hizbollah ORG = ifelse(ORG == &quot;Hizbollah&quot;, &quot;Hezbollah&quot;, ORG), ) 11.2 Standardize to hits pr. article and grouping by month In order to compare the two newspapers we need to standardize the count according to the number of articles. We will make a new variable called penalized_count for both org_nyt and org_guardian. We start by adding a new column year to org_nyt and org_guardian. org_nyt &lt;- org_nyt %&gt;% mutate(month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) org_guardian &lt;- org_guardian %&gt;% mutate(month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) Then we load the dataset containing articles for both newspapers as df_nyt and df_guardian. df1 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) df3 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;) df &lt;- rbind(df1, df2, df3) df_nyt &lt;- as_tibble(df) df_guardian &lt;- read_csv(&quot;data/guardian/guardian_clean.csv&quot;) Here we add a new column to each dataset called month . df_nyt &lt;- df_nyt %&gt;% mutate(month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) df_guardian &lt;- df_guardian %&gt;% mutate(month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) Then we add a new column sum_articles_month which is the sum of articles for a given year. df_nyt &lt;- df_nyt %&gt;% group_by(month) %&gt;% tally() %&gt;% rename(sum_articles_month = n) df_guardian &lt;- df_guardian %&gt;% group_by(month) %&gt;% tally() %&gt;% rename(sum_articles_month = n) For the organisations datasets,We group by organisation and year and summarize the count. So we get a score for each country for each month. org_nyt &lt;- org_nyt %&gt;% group_by(ORG, month) %&gt;% summarise(count = sum(count)) ## `summarise()` has grouped output by &#39;ORG&#39;. You can override using the `.groups` argument. org_guardian &lt;- org_guardian %&gt;% group_by(ORG, month) %&gt;% summarise(count = sum(count)) ## `summarise()` has grouped output by &#39;ORG&#39;. You can override using the `.groups` argument. Then we merge df_nyt with org_nyt so that we now have a column in org_nyt called sum_articles_month which indicates the total number of articles published in the given month. I do the same for df_guardian and org_guardian. org_nyt &lt;- left_join(org_nyt, df_nyt, by = &quot;month&quot;) org_guardian &lt;- left_join(org_guardian, df_guardian, by = &quot;month&quot;) Then we make the columns penalized_count and penalized_count_round which indicates the number of hits for a given person pr. month pr. total articles in that month.We also make a column newspaper to indicate the newspaper. Some months have very few articles, so we also filter out out these, because they can make unreliable values of penalized_count. org_nyt &lt;- org_nyt %&gt;% mutate( penalized_count = count/sum_articles_month, penalized_count_round = round(penalized_count, 1), newspaper = &quot;New York Times&quot; ) %&gt;% filter(sum_articles_month &gt; 30) org_guardian &lt;- org_guardian %&gt;% mutate( penalized_count = count/sum_articles_month, penalized_count_round = round(penalized_count, 1), newspaper = &quot;The Guardian&quot; ) %&gt;% filter(sum_articles_month &gt; 30) Then i bind the datasets together so we can plot them in the same plot and select the columns needed. df_org &lt;- rbind(org_nyt, org_guardian) df_org &lt;- as_tibble(df_org) Now we have a dataset that is ready for plots. 11.3 Color Palettes Here I quickly define some color palettes with colors that i like. These palettes will be used for different plots. The same colors will be used consistently throughout the notebook. color_palette_newspaper &lt;- c(wes_palette(&quot;Chevalier1&quot;)[1], wes_palette(&quot;Darjeeling2&quot;)[2]) 11.4 Summary plot summary_nyt &lt;- df_org %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% filter(ORG %in% c(&quot;NATO&quot;, &quot;Taliban&quot;, &quot;Qaeda&quot;, &quot;Hamas&quot;, &quot;Hezbollah&quot;)) %&gt;% group_by(ORG) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/21109) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(ORG, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[1]) + coord_flip() + labs(x= &quot;Organisation&quot;, y=&quot;Hits per Article&quot;, title = &quot;New York Times&quot;) + theme_minimal() + ylim(0,4.2) summary_guardian &lt;- df_org %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% filter(ORG %in% c(&quot;NATO&quot;, &quot;Taliban&quot;, &quot;Qaeda&quot;, &quot;Hamas&quot;, &quot;Hezbollah&quot;)) %&gt;% group_by(ORG) %&gt;% summarise(count = sum(count)) %&gt;% mutate(hits_pr_article = count/9609) %&gt;% arrange(desc(hits_pr_article)) %&gt;% ggplot() + aes(x=reorder(ORG, hits_pr_article), y=hits_pr_article) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[2]) + coord_flip() + labs(x= &quot;Organisation&quot;, y=&quot;Hits per Article&quot;, title = &quot;The Guardian&quot;) + theme_minimal() + ylim(0,4.2) summary_nyt / summary_guardian + plot_annotation(title = &quot;Organisations&quot;, theme = theme(plot.title = element_text(size=18))) 11.5 NATO nato &lt;- df_org %&gt;% filter(ORG == &quot;NATO&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;NATO&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;2000-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) nato 11.6 Taliban taliban &lt;- df_org %&gt;% filter(ORG == &quot;Taliban&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Taliban&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;2000-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) taliban 11.7 Al Qaeda qaeda &lt;- df_org %&gt;% filter(ORG == &quot;Qaeda&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Al Qaeda&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;2000-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) qaeda 11.8 Hamas hamas &lt;- df_org %&gt;% filter(ORG == &quot;Hamas&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Hamas&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;1999-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) hamas 11.9 Hezbollah hezbollah &lt;- df_org %&gt;% filter(ORG == &quot;Hezbollah&quot;) %&gt;% ggplot() + aes(x=month, y=penalized_count, color = newspaper) + geom_point(size = 0.9) + geom_smooth(alpha=0.2, size = 1.2, method = &quot;gam&quot;) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Hezbollah&quot;) + facet_wrap(~newspaper, nrow = 2) + scale_x_date(limits = as.Date(c(&quot;1999-01-01&quot;, &quot;2021-11-01&quot;))) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) hezbollah 11.10 Combined Plots for all Key Organisations 11.10.1 New York Times All organizations for New York Times. nato &lt;- df_org %&gt;% filter(ORG == &quot;NATO&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( organisation = &quot;NATO&quot; ) taliban &lt;- df_org %&gt;% filter(ORG == &quot;Taliban&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( organisation = &quot;Taliban&quot; ) qaeda &lt;- df_org %&gt;% filter(ORG == &quot;Qaeda&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( organisation = &quot;Al Qaeda&quot; ) hamas &lt;- df_org %&gt;% filter(ORG == &quot;Hamas&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( organisation = &quot;Hamas&quot; ) hezbollah &lt;- df_org %&gt;% filter(ORG == &quot;Hezbollah&quot;) %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( organisation = &quot;Hezbollah&quot; ) pres &lt;- rbind(nato, taliban, qaeda, hamas, hezbollah) pres &lt;- as_tibble(pres) %&gt;% mutate(organisation = as.factor(organisation)) pres &lt;- pres %&gt;% mutate(organisation = fct_relevel(organisation, c(&quot;NATO&quot;, &quot;Taliban&quot;, &quot;Al Queda&quot;, &quot;Hamas&quot;))) pres_nyt &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = organisation) + geom_point(size = 0.5) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Organisations Mentioned in New York Times&quot;) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;8 years&quot;, date_labels = &quot;%Y&quot;) + theme_minimal() + scale_color_manual(values = wes_palette(name = &quot;Darjeeling1&quot;), name = &quot;&quot;) pres_nyt 11.10.2 The Guardian All organizations for the Guardian. nato &lt;- df_org %&gt;% filter(ORG == &quot;NATO&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( organisation = &quot;NATO&quot; ) taliban &lt;- df_org %&gt;% filter(ORG == &quot;Taliban&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( organisation = &quot;Taliban&quot; ) qaeda &lt;- df_org %&gt;% filter(ORG == &quot;Qaeda&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( organisation = &quot;Al Qaeda&quot; ) hamas &lt;- df_org %&gt;% filter(ORG == &quot;Hamas&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( organisation = &quot;Hamas&quot; ) hezbollah &lt;- df_org %&gt;% filter(ORG == &quot;Hezbollah&quot;) %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( organisation = &quot;Hezbollah&quot; ) pres &lt;- rbind(nato, taliban, qaeda, hamas, hezbollah) pres &lt;- as_tibble(pres) %&gt;% mutate(organisation = as.factor(organisation)) pres &lt;- pres %&gt;% mutate(organisation = fct_relevel(organisation, c(&quot;NATO&quot;, &quot;Taliban&quot;, &quot;Al Qaeda&quot;, &quot;Hamas&quot;, &quot;Hezbollah&quot;))) pres_guardian &lt;- pres %&gt;% ggplot() + aes(x=month, y=penalized_count, color = organisation) + geom_point(size = 0.5) + geom_smooth(method=&quot;loess&quot;, se = F, size = 1.2) + labs(y=&quot;Hits per Article&quot;, x = &quot;Date&quot;, title = &quot;Organisations Mentioned in The Guardian&quot;) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;8 years&quot;, date_labels = &quot;%Y&quot;) + theme_minimal() + scale_color_manual(values = wes_palette(name = &quot;Darjeeling1&quot;), name = &quot;&quot;) pres_guardian 11.10.2.1 Both Newspapers Without Facet Wrap pres_nyt_y &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,14) pres_guardian_y &lt;- pres_guardian + theme(legend.position=&quot;bottom&quot;) + ylim(-0.5,14) #combining into single plot pres_both_newspapers &lt;- pres_nyt_y / pres_guardian_y #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers 11.10.2.2 Both Newspapers With Facet Wrap pres_nyt &lt;- pres_nyt + theme(legend.position = &quot;none&quot;) + ylim(-0.5,10) + facet_wrap(~organisation, nrow = 1) + labs(title = &quot;New York Times&quot;) pres_guardian &lt;- pres_guardian + theme(legend.position=&quot;none&quot;) + ylim(-0.5,10) + facet_wrap(~organisation, nrow = 1) + labs(title = &quot;The Guardian&quot;) #combining into single plot pres_both_newspapers &lt;- pres_nyt / pres_guardian #removing axis text from the first plot pres_both_newspapers[[1]] = pres_both_newspapers[[1]] + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank() ) pres_both_newspapers + plot_annotation(title = &quot;Timeline of Organisations&quot;, theme = theme(plot.title = element_text(size=18))) References "],["nouns-verbs-and-adjectives-vizualisation.html", "Chapter 12 Nouns, Verbs and Adjectives Vizualisation 12.1 Color Palettes 12.2 Top 5 Nouns 12.3 Top 5 Verbs 12.4 Top 5 Adjectives", " Chapter 12 Nouns, Verbs and Adjectives Vizualisation In this Chapter I will make visualizations of key nouns, verbs and adjectives. I compare the two newspapers to see if their key nouns, verbs and adjectives differ. The code in this chapter is mainly for making plots. The commenting will be sparse as it would be too tedious to comment everything. First thing first, we start by loading the package tidyverse (Wickham et al. 2019), wesanderson (Ram and Wickham 2018) and patchwork (Pedersen 2020). pacman::p_load(tidyverse, patchwork, wesanderson) Then we load the dataset containing nouns, verbs and adjectives for both newspapers. #NYT noun_nyt_1 &lt;- read_csv(&quot;data/new_york_times/data_NER/noun_1.csv&quot;) noun_nyt_2 &lt;- read_csv(&quot;data/new_york_times/data_NER/noun_2.csv&quot;) noun_nyt &lt;- rbind(noun_nyt_1, noun_nyt_2) noun_nyt &lt;- as_tibble(noun_nyt) verb_nyt &lt;- read_csv(&quot;data/new_york_times/data_NER/verb.csv&quot;) adjective_nyt &lt;- read_csv(&quot;data/new_york_times/data_NER/adjective.csv&quot;) #The Guardian noun_guardian &lt;- read_csv(&quot;data/guardian/data_NER/noun.csv&quot;) verb_guardian &lt;- read_csv(&quot;data/guardian/data_NER/verb.csv&quot;) adjective_guardian &lt;- read_csv(&quot;data/guardian/data_NER/adjective.csv&quot;) 12.1 Color Palettes Here I quickly define some color palettes with colors that i like. These palettes will be used for different plots. The same colors will be used consistently throughout the notebook. color_palette_newspaper &lt;- c(wes_palette(&quot;Chevalier1&quot;)[1], wes_palette(&quot;Darjeeling2&quot;)[2]) 12.2 Top 5 Nouns Here we will plot the top 5 nouns. The two newspapers have a different number of articles. Therefore we need to penalize the number of hits of nouns to the total number of articles. ye &lt;- noun_nyt %&gt;% group_by(noun) %&gt;% summarize(n = sum(count)) %&gt;% mutate(penalized_count = n/21109) %&gt;% arrange(desc(penalized_count)) %&gt;% filter(noun %in% head(unique(noun), 5)) %&gt;% ggplot() + aes(reorder(noun, penalized_count), penalized_count) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[1]) + coord_flip() + labs(x= &quot;Noun&quot;, y=&quot;Hits per Article&quot;, title = &quot;New York Times&quot;) + theme_minimal() yeye &lt;- noun_guardian %&gt;% group_by(noun) %&gt;% summarize(n = sum(count)) %&gt;% mutate(penalized_count = n/9609) %&gt;% arrange(desc(penalized_count)) %&gt;% filter(noun %in% head(unique(noun), 5)) %&gt;% ggplot() + aes(reorder(noun, penalized_count), penalized_count) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[2]) + coord_flip() + labs(x= &quot;Noun&quot;, y=&quot;Hits per Article&quot;, title = &quot;The Guardian&quot;) + theme_minimal() ye/yeye + ylim(0,2.6) + plot_annotation(title = &quot;Top 5 Nouns&quot;, theme = theme(plot.title = element_text(size=18))) 12.3 Top 5 Verbs Here we will plot the top 5 verbs The two newspapers have a different number of articles. Therefore we need to penalize the number of hits of verbs to the total number of articles. ye &lt;- verb_nyt %&gt;% filter(verb != &quot;say&quot;) %&gt;% group_by(verb) %&gt;% summarize(n = sum(count)) %&gt;% mutate(penalized_count = n/21109) %&gt;% arrange(desc(penalized_count)) %&gt;% filter(verb %in% head(unique(verb), 5)) %&gt;% ggplot() + aes(reorder(verb, penalized_count), penalized_count) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[1]) + coord_flip() + labs(x= &quot;Verb&quot;, y=&quot;Hits per Article&quot;, title = &quot;New York Times&quot;) + theme_minimal() yeye &lt;- verb_guardian %&gt;% filter(verb != &quot;say&quot;) %&gt;% group_by(verb) %&gt;% summarize(n = sum(count)) %&gt;% mutate(penalized_count = n/9609) %&gt;% arrange(desc(penalized_count)) %&gt;% filter(verb %in% head(unique(verb), 5)) %&gt;% ggplot() + aes(reorder(verb, penalized_count), penalized_count) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[2]) + coord_flip() + labs(x= &quot;Verb&quot;, y=&quot;Hits per Article&quot;, title = &quot;The Guardian&quot;) + theme_minimal() ye/yeye + ylim(0,2) + plot_annotation(title = &quot;Top 5 Verbs&quot;, theme = theme(plot.title = element_text(size=18))) 12.4 Top 5 Adjectives Here we will plot the top 5 adjectives The two newspapers have a different number of articles. Therefore we need to penalize the number of hits of adjectives to the total number of articles. ye &lt;- adjective_nyt %&gt;% group_by(adjective) %&gt;% summarize(n = sum(count)) %&gt;% mutate(penalized_count = n/21109) %&gt;% arrange(desc(penalized_count)) %&gt;% filter(adjective %in% head(unique(adjective), 5)) %&gt;% ggplot() + aes(reorder(adjective, penalized_count), penalized_count) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[1]) + coord_flip() + labs(x= &quot;Adjective&quot;, y=&quot;Hits per Article&quot;, title = &quot;New York Times&quot;) + theme_minimal() yeye &lt;- adjective_guardian %&gt;% group_by(adjective) %&gt;% summarize(n = sum(count)) %&gt;% mutate(penalized_count = n/9609) %&gt;% arrange(desc(penalized_count)) %&gt;% filter(adjective %in% head(unique(adjective), 5)) %&gt;% ggplot() + aes(reorder(adjective, penalized_count), penalized_count) + geom_bar(stat = &quot;identity&quot;, fill = color_palette_newspaper[2]) + coord_flip() + labs(x= &quot;Adjective&quot;, y=&quot;Hits per Article&quot;, title = &quot;The Guardian&quot;) + theme_minimal() ye/yeye + ylim(0,1.7) + plot_annotation(title = &quot;Top 5 Adjectives&quot;, theme = theme(plot.title = element_text(size=18))) References "],["overview-2.html", "Overview", " Overview We are going to make a simple sentiment analysis which is the process of detecting positive or negative sentiment in text. This is another huge topic in NLP and many advanced analysis can be done but this is beyond the scope of this project. We are going to extract two key features polarity and subjectivity of each article Polarity is a float within the range [-1.0, 1.0] where -1.0 is negative and 1.0 is positive. Subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. The goal is to make visualizations over time showing if certain periods have been more or less negative. "],["sentiment-analysis.html", "Chapter 13 Sentiment Analysis", " Chapter 13 Sentiment Analysis div.blue { background-color:#93c47d; border-radius: 5px; padding: 20px;} This chapter is written in Python. To see the original file go to the folder python_scripts/. I am going to do a simple sentiment analysis using the libary spaCyTextBlob. We start by importing packages, data and nlp pipeline. import spacy import numpy as np import pandas as pd from spacytextblob.spacytextblob import SpacyTextBlob #loading the dataframes and combining them df1 = pd.read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 = pd.read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) df3 = pd.read_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;) #combining df = pd.concat([df1, df2, df3]) #loading small english model nlp = spacy.load(&quot;en_core_web_sm&quot;) Then we add a pipe to the nlp-pipeline, so it can perform sentiment analysis. nlp.add_pipe(&#39;spacytextblob&#39;) Now we are set to go. We iterate though all the articles and calculate the polarity and subjectivity for each article respectively. This should look somewhat familiar. #defining index to loop over article_index = 0 #defining lists for polarity and subjectivity all_polarity = [] all_subjectivity = [] #looping over bread texts from all articles and running the functions on them for article in df.index: #load article using index article = df.iloc[article_index,4] #appending polarity and subjectivity all_polarity.append(article._.polarity) all_subjectivity.append(article._.subjectivity) #indexing to next article article_index += 1 #checking the progress print(article_index) #making new columns in df for polarity and subjectibity respectively df[&#39;polarity&#39;] = all_polarity df[&#39;subjectivity&#39;] = all_subjectivity We finish up by saving the data as three datasets #splitting into three datasets df1 = df.iloc[0:7500,] df2 = df.iloc[7501:15000,] df3 = df.iloc[15001:21113,] #saving to three new files df1.to_csv(&quot;data/new_york_times/NYT_clean1.csv&quot;, index = False) df2.to_csv(&quot;data/new_york_times/NYT_clean2.csv&quot;, index = False) df3.to_csv(&quot;data/new_york_times/NYT_clean3.csv&quot;, index = False) "],["sentiment-vizualisations.html", "Chapter 14 Sentiment Vizualisations 14.1 Data wrangling 14.2 Color Palettes 14.3 Polarity 14.4 Subjectivity", " Chapter 14 Sentiment Vizualisations In this Chapter I will make visualizations on the differences between newspapers on the scores of polarity and subjectivity. The code in this chapter is mainly for making plots. The commenting will be sparse as it would be too tedious to comment everything. First thing first, we start by loading the package tidyverse (Wickham et al. 2019), patchwork (Pedersen 2020) and wesanderson (Ram and Wickham 2018). pacman::p_load(tidyverse, patchwork, wesanderson) Then we load the data #NYT df1 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) df3 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;) df &lt;- rbind(df1, df2, df3) df_NYT &lt;- as_tibble(df) #The Guardian df_guardian &lt;- read_csv(&quot;data/guardian/guardian_clean.csv&quot;) 14.1 Data wrangling Some wrangling is needed to make the plots. Here we add three new columns to each dataset: newspaper indicating which newspaper the article is from. We also add month. . df_NYT &lt;- df_NYT %&gt;% mutate(newspaper = &quot;New York Times&quot;, month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) df_guardian &lt;- df_guardian %&gt;% mutate(newspaper = &quot;The Guardian&quot;, month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) Then we add two new columns polarity_month and subjectivity_month which is the mean polarity and subjectivety of the given month. This is done by grouping articles by month, taking the mean of polarity and binding this column to the original dataset. #grouping and summarizing df_NYT_month &lt;- df_NYT %&gt;% group_by(month) %&gt;% summarize(polarity_month = mean(polarity), subjectivity_month = mean(subjectivity)) df_guardian_month &lt;- df_NYT %&gt;% group_by(month) %&gt;% summarize(polarity_month = mean(polarity), subjectivity_month = mean(subjectivity)) #binding df_NYT &lt;- left_join(df_NYT, df_NYT_month) ## Joining, by = &quot;month&quot; df_guardian &lt;- left_join(df_guardian, df_guardian_month) ## Joining, by = &quot;month&quot; Then we make a dataset with both df_NYT and df_guardian so we can plot both newspapers in the same plot. df_both &lt;- rbind(df_NYT, df_guardian) df_both &lt;- as_tibble(df_both) Basta. 14.2 Color Palettes Here I quickly define some color palettes with colors that i like. These palettes will be used for different plots. The same colors will be used consistently throughout the notebook. color_palette_newspaper &lt;- c(wes_palette(&quot;Chevalier1&quot;)[1], wes_palette(&quot;Darjeeling2&quot;)[2]) 14.3 Polarity Polarity is a float within the range [-1.0, 1.0] where -1.0 is negative and 1.0 is positive. 14.3.1 Summary stats We start by doing some quick summary stats to see if the polarity differ between newspapers. mean(df_NYT$polarity) ## [1] 0.02037221 mean(df_guardian$polarity) ## [1] 0.04714502 df_both %&gt;% ggplot() + aes(x=polarity, fill = newspaper) + geom_density(alpha = 0.5) + geom_vline(xintercept = mean(df_NYT$polarity), color = color_palette_newspaper[1]) + geom_vline(xintercept = mean(df_guardian$polarity), color = color_palette_newspaper[2]) + theme_minimal() + scale_fill_manual(values = color_palette_newspaper, name = &quot;&quot;) + labs(x=&quot;Polarity&quot;, y=&quot;Density&quot;, title=&quot;Distributions of Polarity&quot;) + theme(legend.position=&quot;bottom&quot;) The mean polarity of the articles in New York Times is 0.0204. The mean polarity of the articles in The Guardian is 0.0471. Both Newspapers have a positive Polarity. The values of polarity are quite small so the difference is small. Nonetheless mean polarity of the articles in The Guardian is actually twice as large as the mean polarity in New York Times. However, taking a look at ?? there is hadly a difference between the two distributions. We could make some bayesian statistics, but this is beyond the scope of this project. 14.3.2 Timeline q &lt;- df_both %&gt;% ggplot() + aes(x=month, color = newspaper) + geom_smooth(method = &quot;gam&quot;, aes(y=polarity_month), size = 1.2) + geom_point(aes(y=polarity_month), size = 0.3) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;3 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~newspaper, nrow = 2) + labs(x=&quot;Date&quot;, y = &quot;Polarity&quot;, title = &quot;Timeline of Polarity&quot;) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) q 14.4 Subjectivity Subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. 14.4.1 Summary stats We start by doing some quick summary stats to see if the subjectivity differ between newspapers. mean(df_NYT$subjectivity) ## [1] 0.383274 mean(df_guardian$subjectivity) ## [1] 0.3924738 df_both %&gt;% ggplot() + aes(x=subjectivity, fill = newspaper) + geom_density(alpha = 0.5) + geom_vline(xintercept = mean(df_NYT$subjectivity), color = color_palette_newspaper[1]) + geom_vline(xintercept = mean(df_guardian$subjectivity), color = color_palette_newspaper[2]) + theme_minimal() + scale_fill_manual(values = color_palette_newspaper, name = &quot;&quot;) + labs(x=&quot;Subjectivity&quot;, y=&quot;Density&quot;, title=&quot;Distributions of Subjectivity&quot;) + theme(legend.position=&quot;bottom&quot;) The mean subjectivity of the articles in New York Times is 0.383. The mean subjectivity of the articles in The Guardian is 0.392. Both Newspapers have a medium subjectivity, meaning that they in between objective and non-objective. There is no real difference between the mean values of subjectivity for the two newspapers. Taking a look at ?? there is hardly a difference between the two distributions. We could make some bayesian statistics, but this is beyond the scope of this project. 14.4.2 Timeline q &lt;- df_both %&gt;% ggplot() + aes(x=month, color = newspaper) + geom_smooth(method = &quot;gam&quot;, aes(y=subjectivity_month), size = 1.2) + geom_point(aes(y=subjectivity_month), size = 0.3) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;3 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~newspaper, nrow = 2) + labs(x=&quot;Date&quot;, y = &quot;Subjectivity&quot;, title = &quot;Timeline of Subjectivity&quot;) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) q References "],["overview-3.html", "Overview What is an LDA topic model? Example The goal of LDA topic model", " Overview What is an LDA topic model? Next step on the agenda is something called LDA topic modeling. Topic modeling is a type of statistical modeling for discovering topics that occur in a collection of documents. If we have a large collection of news articles some of them are more similar to others. Topic modelling cluster together articles that are similar under an umbrella called a topic. Latent Dirichlet Allocation (LDA) is an example of a topic model. Latent means that there are hidden topics in the articles. Dirichlet refers to a Dirichlet distribution. We have two dirichlet distributions: A distribution associating articles with topics. Each article is assigned a probability of belonging to a certain topic. A distribution associating topics with words. Each topic is assigned a probability of containing a certain word. These two distributions are put together to generate a fake article. This fake article is compared to the actual article to see how similar they are. Allocation is the process of allocating topics to documents and words of the documents to topics. Example Lets say we create an LDA topic model with 3 topics: Science, politics and sports. In this model each article is assigned 3 probabilities; one probability for each topic. Lets say that article x has the following probabilities of belonging to the 3 topics: Science: 0.02 Politics: 0.03 Sports: 0.95 Article x clearly belongs more to the topic sports than to the other topics. Lets say that article y has the following probabilities of belonging to the 3 topics: Science: 0.45 Politics: 0.10 Sports: 0.45 Now, it is more difficult which topic article y belongs to. The goal of this example is to illustrate that each article in an LDA topic model is comprised of a distribution of topics. And some articles have clear distributions, making them belong more clearly to a single topic whereas other articles are more difficult to place inside a single topic. The goal of LDA topic model The goal of topic modeling is to assign a topic to each article. Then we want to make visualizations comparing the topics in New York Times to the topics in The Guardian. We will also make a timeline showing when certain topics have been more or less prevalent. This will hopefully give an overview of certain trends in both newspapers. "],["lda-topic-modelling.html", "Chapter 15 LDA Topic Modelling 15.1 Packages and Data 15.2 Preprocessing 15.3 Bigrams and Trigrams 15.4 TF-IDF Removal 15.5 Base topic model 15.6 Selecting the number of topics based on coherence score 15.7 Building the topic models 15.8 Assigning one topic to each article", " Chapter 15 LDA Topic Modelling div.green { background-color:#93c47d; border-radius: 5px; padding: 20px;} Part of this chapter is written in Python. To see the original file go to the folder python_scripts/. div.blue { background-color:#76a5af; border-radius: 5px; padding: 20px;} All the code in the chapter is run on both df_NYT and df_guardian but some of the code is only shown for df_nyt. In this Chapter we are going to make a LDA topic model for the dataset df_nyt and df_guardian. 15.1 Packages and Data Lets get to the code. We load a bunch of packages. Importantly, we will use a new library called gensim which is used for topic modelling. We also use spaCy and pyLDAvis which is used for vizualizing the model output. import numpy as np import pandas as pd import glob #Gensim import gensim import gensim.corpora as corpora from gensim.utils import simple_preprocess from gensim.models import CoherenceModel from gensim.models import TfidfModel from gensim.models import CoherenceModel #spacy import spacy #vis import pyLDAvis import pyLDAvis.gensim_models Then we load the dataset containing the articles. df1 = pd.read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 = pd.read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) df3 = pd.read_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;) df = pd.concat([df1, df2, df3]) 15.2 Preprocessing Although the bread text of all the articles is already preprocessed (lemmatization, removal of stopwords and removal of punctuation) we still need a little bit of preprocessing to make the LDA topic model. Here we define a helper function that break down the articles by individual words and apply a function called simple_preprocess from gensim. def gen_words(texts): final = [] for text in texts: new = gensim.utils.simple_preprocess(text) final.append(new) return (final) We use the function on the articles from the dataset, to generate a list of all the articles broken down into words called data_words. data_words = gen_words(df[&quot;articles_clean&quot;]) Now lets see what this helper function actually did. print(&quot;Before applying the helper function gen_words: \\n&quot; + str(df.iloc[0,4][0:142])) print(&quot;\\nAfter applying the helper function gen_words: \\n&quot; + str(data_words[0][0:20])) Gensim Preprocessing This is how the data looks before and after applying the helper function gen_words. Before application we see that the data is simply a collection of words. After application we see that all the words have been split into their element in a list. So now each article is a list where the elements of that list is the individual words. 15.3 Bigrams and Trigrams Next up we make bigrams and trigrams. Bigrams are 2 consecutive word in a sentence that occur with a high frequency. Trigrams are 3 consecutive words in a sentence that occur with a high frequency. Lets take an example where we have the following sentences: The connection of devices is wireless. The speakers have a solid bass. The wireless speakers are expensive. In sentence 1 the word wireless occurs by itself. Similarly, in sentence 2 the word speakers occur by itself. However, in sentence 3 the two words occur together in a meaningful way to form a single unit. If wireless speakers occurs with a high enough frequency, we call it a bigram. Now we get to the code. We look for bigrams using the function Phrases from the library gensim. This function takes data_words which we created earlier and min_count which determines the minimum number of times two words need to occur together to be considered a bigram. It also takes threshold which determines the the number of phrases that are found. A higher threshold will result in fewer bigrams. It is important to adjust the threshold so it doesnt pick up too few or too many bigrams. We also look for trigrams using the same function but this time using the bigram_phrases as input. bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=80) trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=80) Then we create the objects bigram and trigram. bigram = gensim.models.phrases.Phraser(bigram_phrases) trigram = gensim.models.phrases.Phraser(trigram_phrases) Next we make two functions that add the bigrams and trigrams into data_words. def make_bigrams(texts): return([bigram[doc] for doc in texts]) def make_trigrams(texts): return ([trigram[bigram[doc]] for doc in texts]) Here we apply the functions. data_bigrams = make_bigrams(data_words) data_bigrams_trigrams = make_trigrams(data_bigrams) Now lets see which bigrams and trigrams have been picked up. print(&quot;Without bigrams and trigrams:\\n&quot; + str(data_words[1067][0:100])) print (&quot;\\nWith bigrams and trigrams:\\n&quot; + str(data_bigrams_trigrams[1067][0:100])) Bigrams and Trigrams In the first section without bigrams and trigrams we see that the data appears as individual units. In the second section we see the same words as before but we also see bigrams and trigrams highlighted by a yellow color. 15.4 TF-IDF Removal TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It is a way of ranking how important a word is to a document in a collection of documents. TF-IDF is short for term frequency - inverse document frequency. Term frequency is the frequency of a word in a document. You simply count how many times a word appears in a document. Inverse document frequency indicates how common or rare a word is in the entire collection of documents. The closer to 0, the more common a word is in the collection of documents. Multiplying these two numbers results in the TF-IDF score for a word. The higher the score, the more relevant a word is to a particular document. TF-IDF has many applications. Here i use it to remove words that dont add any meaningful value to the topic model. I am going to remove frequently occurring words such as say. The reason we do this is that some words are so generic that they do not add any meaningful information to the topics in the topic model. The result of this removal is that our topics will be more distinct, i.e. there will be less overlap between topics. We start be defining the object texts which is the list of words of all articles containing bigrams and trigrams. texts = data_bigrams_trigrams Then we make a dictionary from texts which counts the occurrence of words in each article. id2word = corpora.Dictionary(texts) We convert all the documents into a bag of words. corpus = [id2word.doc2bow(text) for text in texts] We make the TD-IDF model and a variable called low_value which determines the threshold where words are removed. A higher threshold will result in more words being removed. tfidf = TfidfModel(corpus, id2word=id2word) low_value = 0.03 Here is a large chunk of code, but dont sweat it. It basically looks for words that are so generic across all documents that they dont add any meaningful value to the topic model. The end product is a new corpus where these generic words are removed. words = [] words_missing_in_tfidf = [] for i in range(0, len(corpus)): bow = corpus[i] low_value_words = [] #reinitialize to be safe. You can skip this. tfidf_ids = [id for id, value in tfidf[bow]] bow_ids = [id for id, value in bow] low_value_words = [id for id, value in tfidf[bow] if value &lt; low_value] drops = low_value_words+words_missing_in_tfidf for item in drops: words.append(id2word[item]) words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf score 0 will be missing new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf] corpus[i] = new_bow So now our texts contain bigrams and trigrams and frequently occurring words have been removed. Now to the fun part. 15.5 Base topic model Now we can finally create the LDA topic model using gensim. The two main inputs to the topic model are our dictionary/id2word and corpus which we created earlier. We can adjust many hyperparameters such as random_state and alpha to make the model perform optimally. We can also choose the number of topics in num_topics. Here i just choose 10 topics. lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = id2word, num_topics = 10, random_state = 100, update_every = 1, chunksize = 100, passes = 10, alpha = &quot;auto&quot;, ) Now we can visualize the model. If you want to play around with it yourself go to the original python script. pyLDAvis.enable_notebook() vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds = &quot;mmds&quot;, R=30) vis Base Topic Model for New York Times In the base topic model for New York Times we see that 10 topics have been created. On the left we see topics 1-10 plotted on a 2-dimensional space. The size of the circles indicate the prevalence of the topic throughout the articles. Larger means more prevalent. We see that the topics are well spread throughout the 2-dimensional space and that there is no obvious overlap between topics. On the right we see the top-words for topic number 1. 15.6 Selecting the number of topics based on coherence score There are many ways of evaluating an LDA topic model to see if it performs as we intend it to. Likewise there are many hyperparameters that can evaluated and tuned accordingly such as alpha and beta. Here i will evaluate the model by choosing the number of topics using something called a coherence score. There are many coherence measures, here I use one called C_v. In the next section of code I compute the coherence score for topic models with a varying number of topics to see which number of topics is the optimal. I start by defining a helper function which creates an LDA model. It takes k as argument which is the number of topics. It then adds a coherence measure to the model and returns a coherence score. def compute_coherence_values(k): lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=k, random_state=100, chunksize=100, passes=10, alpha=&quot;auto&quot;) coherence_model_lda = CoherenceModel(model=lda_model, texts=data_bigrams_trigrams, dictionary=id2word, coherence=&#39;c_v&#39;) return coherence_model_lda.get_coherence() Now that we defined a helper function we can iterate over a range of topics, create a topic model for each and calculate a coherence score for each topic model. I used a range of 1:30 topics with a step size of 2. We save the result as a dataframe. # Topics range min_topics = 1 max_topics = 30 step_size = 2 topics_range = range(min_topics, max_topics, step_size) #empty dataframe model_results = {&#39;Topics&#39;: [], &#39;Coherence&#39;: [] } # iterate through number of topics for k in topics_range: print(k) # get the coherence score for the given topics cv = compute_coherence_values(k=k) # Save the model results model_results[&#39;Topics&#39;].append(k) model_results[&#39;Coherence&#39;].append(cv) pd.DataFrame(model_results).to_csv(&#39;data/lda_topic_model/tuning_results_nyt.csv&#39;, index=False) 15.6.1 Selecting number of topics for New York Times We now move into R for a lil bit to make some plots. Here we load packages tidyverse (Wickham et al. 2019), wesanderson (Ram and Wickham 2018) and RColorBrewer (R-rcolorbrewer?). pacman::p_load(tidyverse, wesanderson, RColorBrewer) Here I quickly define some color palettes with colors that i like. These palettes will be used for different plots. The same colors will be used consistently throughout the notebook. color_palette_newspaper &lt;- c(wes_palette(&quot;Chevalier1&quot;)[1], wes_palette(&quot;Darjeeling2&quot;)[2]) Lets plot the coherence score against number of topics for New York Times. lda_tuning_results_NYT &lt;- read_csv(&quot;data/lda_topic_model/tuning_results_NYT.csv&quot;) lda_tuning_results_NYT %&gt;% ggplot() + aes(x=Topics, y=Coherence) + geom_point(color = color_palette_newspaper[1], size = 1.1) + geom_line(color = color_palette_newspaper[1], size = 1.2) + scale_x_continuous(breaks = seq(1, 30, by = 2)) + theme_minimal() + labs(x=&quot;Number of Topics&quot;, y=&quot;Coherence Score&quot;, title = &quot;Choosing Optimal Number of Topics for New York Times&quot;) + ylim(0.38,0.52) (#fig:tuning_nyt)Coherence score plotted for topic models with varying number of topics. 7 Topics are chosen for the topic model in New York Times Figure @ref(fig:tuning_nyt) outlines the coherence score for the number of topics in the topic model on articles from New York Times. We want to pick the value in the graph where there is a breaking point. This is the point where the coherence score is highest before flattening out. In other words we want to pick the lowest number of topics where the coherence score begins to level off. I choose to go with 7 topics for the topic model in New York Times 15.6.2 Selecting number of topics for The Guardian lda_tuning_results_guardian &lt;- read_csv(&quot;data/lda_topic_model/tuning_results_guardian.csv&quot;) lda_tuning_results_guardian %&gt;% ggplot() + aes(x=Topics, y=Coherence) + geom_point(color = color_palette_newspaper[2], size = 1.1) + geom_line(color = color_palette_newspaper[2], size = 1.2) + scale_x_continuous(breaks = seq(1, 30, by = 2)) + theme_minimal() + labs(x=&quot;Number of Topics&quot;, y=&quot;Coherence Score&quot;, title = &quot;Choosing Optimal Number of Topics for The Guardian&quot;) + ylim(0.38,0.52) (#fig:tuning_guardian)Coherence score plotted for topic models with varying number of topics. 7 Topics are chosen for the topic model in The Guardian Figure @ref(fig:tuning_guardian) outlines the coherence score for the number of topics in the topic model on articles from The Guardian. Using the same criteria as above we should choose the number of topics to be 13. However, this seems like too many topics and it will be difficult to compare 13 topics to the 7 topics chosen for New York Times. Therefore I choose to go with 7 topics for the topic model in The Guardian 15.7 Building the topic models Here we build the topic models for New York Times and The Guardian respectively by using the number of topics designated in the previous section. 15.7.1 New York Times topic model We create the final model with the selected number of topics for NYT, changing num_topics to 7. lda_model_nyt = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = id2word, num_topics = 7, random_state = 100, update_every = 1, chunksize = 100, passes = 10, alpha = &quot;auto&quot;, ) We plot it. If you want to play around with it yourself go to the original python script. pyLDAvis.enable_notebook() vis = pyLDAvis.gensim_models.prepare(lda_model_nyt, corpus, id2word, mds = &quot;mmds&quot;, R=30) vis Topic Model for New York Times In the topic model for New York Times we see that 7 topics have been created. On the left we see topics 1-7 plotted on a 2-dimensional space. The size of the circles indicate the prevalence of the topic throughout the articles. Larger means more prevalent. We see that the topics are well spread throughout the 2-dimensional space and that there is no obvious overlap between topics. On the right we see the top-words for topic number 1. I highly encourage you to run the python script yourself and play around with the vizualisation. 15.7.2 The Guardian topic model Same procedure for the guardian, changing the num_topics to 7. lda_model_guardian = gensim.models.ldamodel.LdaModel(corpus = corpus, id2word = id2word, num_topics = 7, random_state = 100, update_every = 1, chunksize = 100, passes = 10, alpha = &quot;auto&quot;, ) pyLDAvis.enable_notebook() vis = pyLDAvis.gensim_models.prepare(lda_model_guardian, corpus, id2word, mds = &quot;mmds&quot;, R=30) vis Topic Model for The Guardian In the topic model for The Guardian we see that 7 topics have been created. On the left we see topics 1-7 plotted on a 2-dimensional space. We see that the topics are well spread throughout the 2-dimensional space and that there is no obvious overlap between topics. On the right we see the top-words for topic number 1. I highly encourage you to run the python script yourself and play around with the vizualisation. 15.8 Assigning one topic to each article Remember that each article in an LDA topic model is comprised of a distribution of topics. Let me elaborate with an example. The topic distribution for article x might look something like this: tibble( Topic = c(1,2,3,4,5,6,7), Topic_contribution = c(0.05, 0.10, 0.30, 0.02, 0.03, 0.05, 0.45) ) %&gt;% knitr::kable(caption = &quot;Example: Topic distribution for article x&quot;, col.names = c(&quot;Topic Number&quot;, &quot;Topic Contribution&quot;)) Table 15.1: Example: Topic distribution for article x Topic Number Topic Contribution 1 0.05 2 0.10 3 0.30 4 0.02 5 0.03 6 0.05 7 0.45 We see that article x is not fit into a single topic but rather that each topic has a probability associated to it. All these probabilities add up to 1. We are going to reduce this dimensionality, such that each article is assigned to the topic which is has the highest probability of belonging to. In essence, we are throwing away the probabilistic nature of the model, but in return we get a format that is easier to handle. In this example article x would be assigned to topic number 7. Now back to the real business. The code below iterates through all the articles in the dataframe and extract the probability of each topic for each article. all_topics = [] #looping through all the articles in df for i in range(len(df_nyt)): #getting the probability and index of each topic for given article top_topics = lda_model_nyt.get_document_topics(corpus[i], minimum_probability=0.0) #checking if there was some lists that were not functioning correctly if len(top_topics) != 7: print(&quot;List of Topics is not 7 in article number &quot; + str(i)) #removing the index of the topic, keep only the probabilities topic_vec_prop = [top_topics[i][1] for i in range(7)] all_topics.append(topic_vec_prop) Then, for each article we find the topic with the highest probability and we also find the probability of that topic. dominant_topic = [] topic_contribution = [] for i in all_topics: max_prop = max(i) max_index = i.index(max_prop) dominant_topic.append(max_index) topic_contribution.append(max_prop) We save the dominant topic and its contribution to two new columns in the dataframe. df[&#39;dominant_topic&#39;] = dominant_topic df[&#39;topic_contribution&#39;] = topic_contribution And we save the dataframe with the added columns. #splitting into three datasets df1 = df.iloc[0:7500,] df2 = df.iloc[7501:15000,] df3 = df.iloc[15001:21109, ] #saving to three new files df1.to_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;, index = False) df2.to_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;, index = False) df3.to_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;, index = False) 15.8.1 Evaluating the assignment of one article All right now we have assigned one topic to each article. This format is easy to handle but it throws away a lot of information from the topic model. As described in the example in section ?? some articles have clear distributions, making them belong more clearly to a single topic whereas other articles are more difficult to place inside a single topic. We basically want to articles to have a clear distribution, showing a clear belonging to a single topic. We get a sense of how well the articles fit into a single topic by making two plots: Plotting the distribution of topic_contribution which is the probability of the dominant topic. Plotting the distribution of topic_contribution for each topic, to see if certain topics are more problematic than others. We start by loading the data for both NYT and The Guardian. #NYT df1 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) df3 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;) df &lt;- rbind(df1, df2, df3) df_NYT &lt;- as_tibble(df) %&gt;% mutate( newspaper = &quot;New York Times&quot; ) #The Guardian df_guardian &lt;- read_csv(&quot;data/guardian/guardian_clean.csv&quot;) %&gt;% mutate( newspaper = &quot;The Guardian&quot; ) #combined df_all &lt;- rbind(df_NYT, df_guardian) df_all &lt;- as_tibble(df_all) Here we plot the distribution of topic_contribution for both datasets. df_all %&gt;% ggplot() + aes(x=topic_contribution, fill = newspaper) + geom_density(alpha = 0.8) + geom_vline(xintercept = mean(df_NYT$topic_contribution), color = color_palette_newspaper[1]) + geom_vline(xintercept = mean(df_guardian$topic_contribution), color = color_palette_newspaper[2]) + theme_minimal() + scale_fill_manual(values = color_palette_newspaper, name = &quot;&quot;) + facet_wrap(~newspaper, nrow = 2) + labs(x = &quot;dominant topic contribution&quot;, title = &quot;Distributions of Dominant Topic Contribution&quot;) + theme(legend.position = &quot;none&quot;) + xlim(0,1) Figure 15.1: Distributions of dominant topic contribution for New York Times and The Guardian respectively. Dominant topic contribution is the association between an article and the highest ranking topic for that article. High values indicate that the article belong more clearly to a single topic. Low values indicate the the article is more ambigious and cannot be placed so clearly within a single topic. The vertical yellow and green line show the mean of dominant topic contribution for the two newspapers respectively. In figure 15.1 we see distributions of dominant topic contribution for New York Times and The Guardian respectively. High values indicate that the article belong more clearly to a single topic. Low values indicate the the article is more ambigious and cannot be placed so clearly within a single topic. The vertical yellow and green line show the mean of dominant topic contribution for the two newspapers respectively. Overall I am pretty satisfied with the results of this. We see that the mean dominant topic contribution is ~.40 which is good considering that there are 7 topics, so a completely ambigious score would be 0.14. However, we do see some values drop to about 0.17 but there are very few of these. Moreover, the mean dominant topic contribution for The Guardian is lower compared to the mean dominant topic contribution New York Times. This means that the article in New York Times on average belong more clearly to a single topic. Next up we plot the distributions of topic_contribution for each topic, to see if certain topics are more ambigious than others. We do this for both newspapers. df_mean &lt;- df_all %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( dominant_topic = dominant_topic + 1 ) %&gt;% group_by(dominant_topic) %&gt;% summarise(mean_topic_contribution = mean(topic_contribution)) df_all_p &lt;- df_all %&gt;% mutate(dominant_topic = dominant_topic + 1, dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic))) df_all_p &lt;- df_all_p %&gt;% mutate(dominant_topic = fct_relevel(dominant_topic, c(&quot;7&quot;, &quot;4&quot;, &quot;2&quot;, &quot;1&quot;, &quot;5&quot;, &quot;3&quot;, &quot;6&quot;))) df_all_p %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate(dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic))) %&gt;% ggplot() + aes(x=topic_contribution, fill = dominant_topic_name) + geom_density(alpha = 0.8) + geom_vline(data = df_mean, mapping = aes(xintercept = mean_topic_contribution)) + theme_minimal() + scale_fill_manual(values = RColorBrewer::brewer.pal(7, &quot;Set2&quot;), name = &quot;&quot;) + facet_wrap(~dominant_topic_name, nrow = 7) + xlim(0,1) + labs(x = &quot;dominant topic contribution&quot;, title = &quot;Distributions of Dominant Topic Contribution pr. Topic&quot;, subtitle = &quot;New York Times&quot;) + theme(legend.position = &quot;none&quot;) df_mean &lt;- df_all %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( dominant_topic = dominant_topic + 1 ) %&gt;% group_by(dominant_topic) %&gt;% summarise(mean_topic_contribution = mean(topic_contribution)) df_all %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( dominant_topic = dominant_topic + 1, dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)) )%&gt;% ggplot() + aes(x=topic_contribution, fill = dominant_topic_name) + geom_density(alpha = 0.8) + geom_vline(data = df_mean, mapping = aes(xintercept = mean_topic_contribution)) + theme_minimal() + scale_fill_manual(values = RColorBrewer::brewer.pal(7, &quot;Set1&quot;), name = &quot;&quot;) + facet_wrap(~dominant_topic, nrow = 7) + xlim(0,1) + labs(x = &quot;dominant topic contribution&quot;, title = &quot;Distributions of Dominant Topic Contribution pr. Topic&quot;, subtitle = &quot;The Guardian&quot;) + theme(legend.position = &quot;none&quot;) References "],["topic-model-interpretation-and-vizualisation.html", "Chapter 16 Topic Model Interpretation and Vizualisation 16.1 Plotting topic prevalence 16.2 Plotting topic prevalence over time 16.3 Does topics differ in polarity and subjectivity?", " Chapter 16 Topic Model Interpretation and Vizualisation Here we load packages tidyverse (Wickham et al. 2019), wesanderson (Ram and Wickham 2018), patchwork (Pedersen 2020) and RColorBrewer (Neuwirth 2014). pacman::p_load(tidyverse, wesanderson, RColorBrewer, patchwork) We also load the data. #NYT df1 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_1.csv&quot;) df2 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_2.csv&quot;) df3 &lt;- read_csv(&quot;data/new_york_times/NYT_clean_3.csv&quot;) df &lt;- rbind(df1, df2, df3) df_NYT &lt;- as_tibble(df) %&gt;% mutate(newspaper = &quot;New York Times&quot;) #The Guardian df_guardian &lt;- read_csv(&quot;data/guardian/guardian_clean.csv&quot;) %&gt;% mutate( newspaper = &quot;The Guardian&quot; ) #combined df_all &lt;- rbind(df_NYT, df_guardian) df_all &lt;- as_tibble(df_all) 16.1 Plotting topic prevalence all_nyt &lt;- df_all %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% group_by(dominant_topic) %&gt;% tally() %&gt;% mutate(count_topic = n/21109) %&gt;% arrange(desc(count_topic)) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)), ) %&gt;% ggplot() + aes(x=reorder(dominant_topic_name, count_topic), y=count_topic) + geom_bar(stat = &quot;identity&quot;, fill = RColorBrewer::brewer.pal(7, &quot;Set2&quot;)) + coord_flip() + labs(x= &quot;Topic&quot;, y=&quot;Proportion of Articles&quot;, title = &quot;New York Times&quot;) + theme_minimal() + ylim(0,0.4) all_guardian &lt;- df_all %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% group_by(dominant_topic) %&gt;% tally() %&gt;% mutate(count_topic = n/9609) %&gt;% arrange(desc(count_topic)) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)), ) %&gt;% ggplot() + aes(x=reorder(dominant_topic_name, count_topic), y=count_topic) + geom_bar(stat = &quot;identity&quot;, fill = RColorBrewer::brewer.pal(7, &quot;Set1&quot;)) + coord_flip() + labs(x= &quot;Topic&quot;, y=&quot;Proportion of Articles&quot;, title = &quot;The Guardian&quot;) + theme_minimal() + ylim(0,0.4) all_nyt/all_guardian + plot_annotation(title = &quot;Proportion of Articles for Each Topic&quot;, theme = theme(plot.title = element_text(size=18))) 16.2 Plotting topic prevalence over time 16.2.1 Data Wrangling Some wrangling is needed to make the plots. Here we add one new column to the dataset called month indicating the month. We also add +1 to all values in dominant_topic because they were 0-indexed from Python. df_all &lt;- df_all %&gt;% mutate(month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month), dominant_topic = dominant_topic + 1) Then we make a new dataframe df_all_month where we group articles by newspaper, month and dominant_topic. We then count the number of articles per newspaper per month per topic. df_all_month &lt;- df_all %&gt;% group_by(newspaper, month, dominant_topic) %&gt;% tally() %&gt;% rename(count_topics = n) We make another dataframe df_all_month_1 where we group articles by newspaper and month. We then count the number of articles per newspaper per month. df_all_month_1 &lt;- df_all %&gt;% group_by(newspaper, month) %&gt;% tally() %&gt;% rename(count_total = n) We merge the two dataframes together and add a new column topic_contribution_proportion. This column indicates the proportion of articles that belongs to a given topic in a given month for a given newspaper. df_all_month_2 &lt;- left_join(df_all_month, df_all_month_1) ## Joining, by = c(&quot;newspaper&quot;, &quot;month&quot;) df_all_month_2 &lt;- df_all_month_2 %&gt;% mutate(topic_proportion = count_topics/count_total) 16.2.2 Plots q &lt;- df_all_month_2 %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% filter(month &gt; as.Date(&quot;1999-01-01&quot;)) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)), ) %&gt;% ggplot() + aes(x=month, y = topic_proportion, color = dominant_topic_name) + geom_smooth(aes(fill = dominant_topic), method = &quot;gam&quot;, size = 1.2) + geom_point(aes(fill = dominant_topic), size = 0.3) + ylim(0, 1) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + labs(x=&quot;Date&quot;, y=&quot;Proportion of Articles&quot;, title = &quot;Topic Prevalence for The Guardian&quot;) + theme_minimal() + scale_fill_manual(values = RColorBrewer::brewer.pal(7, &quot;Set2&quot;), name = &quot;&quot;) + facet_wrap(~dominant_topic_name, nrow = 7) + theme(legend.position = &quot;none&quot;) q q &lt;- df_all_month_2 %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% filter(month &gt; as.Date(&quot;1999-01-01&quot;)) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)) ) %&gt;% ggplot() + aes(x=month, color = dominant_topic_name) + geom_smooth(aes(y=topic_proportion, fill = dominant_topic), method = &quot;gam&quot;, size = 1.2) + geom_point(aes(y=topic_proportion, fill = dominant_topic), size = 0.3) + ylim(0, 1) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;))) + labs(x=&quot;Date&quot;, y=&quot;Proportion of Articles&quot;, title = &quot;Topic Prevalence for The Guardian&quot;) + theme_minimal() + scale_fill_manual(values = RColorBrewer::brewer.pal(7, &quot;Set2&quot;), name = &quot;&quot;) + facet_wrap(~dominant_topic_name, nrow = 7) + theme(legend.position = &quot;none&quot;) q 16.3 Does topics differ in polarity and subjectivity? 16.3.1 Polarity all_nyt &lt;- df_all %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% group_by(dominant_topic) %&gt;% summarize(mean_polarity = mean(polarity)) %&gt;% arrange(desc(mean_polarity)) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)), ) %&gt;% ggplot() + aes(x=reorder(dominant_topic_name, mean_polarity), y=mean_polarity) + geom_bar(stat = &quot;identity&quot;, fill = RColorBrewer::brewer.pal(7, &quot;Set2&quot;)) + coord_flip() + labs(x= &quot;Topic&quot;, y=&quot;Mean polarity&quot;, title = &quot;New York Times&quot;) + theme_minimal() + ylim(-0.015,0.07) all_guardian &lt;- df_all %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% group_by(dominant_topic) %&gt;% summarize(mean_polarity = mean(polarity)) %&gt;% arrange(desc(mean_polarity)) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)), ) %&gt;% ggplot() + aes(x=reorder(dominant_topic_name, mean_polarity), y=mean_polarity) + geom_bar(stat = &quot;identity&quot;, fill = RColorBrewer::brewer.pal(7, &quot;Set1&quot;)) + coord_flip() + labs(x= &quot;Topic&quot;, y=&quot;Mean polarity&quot;, title = &quot;The Guardian&quot;) + theme_minimal() + ylim(-0.015,0.07) all_nyt/all_guardian + plot_annotation(title = &quot;Mean Polarity for Each Topic&quot;, theme = theme(plot.title = element_text(size=18))) bl &lt;- df_all %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)) ) %&gt;% ggplot() + aes(x=polarity, fill = dominant_topic_name) + geom_density(alpha = 0.5) + theme_minimal() + scale_fill_manual(values = RColorBrewer::brewer.pal(7, &quot;Set2&quot;), name = &quot;&quot;) + labs(x=&quot;Polarity&quot;, y=&quot;Density&quot;, title=&quot;New York Times&quot;) + facet_wrap(~dominant_topic_name, nrow=7) + xlim(-0.3,0.3) + theme(legend.position = &quot;bottom&quot;) bll &lt;- df_all %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)) ) %&gt;% ggplot() + aes(x=polarity, fill = dominant_topic_name) + geom_density(alpha = 0.5) + theme_minimal() + scale_fill_manual(values = RColorBrewer::brewer.pal(7, &quot;Set1&quot;), name = &quot;&quot;) + labs(x=&quot;Polarity&quot;, y=&quot;Density&quot;, title=&quot;The Guardian&quot;) + facet_wrap(~dominant_topic_name, nrow=7) + xlim(-0.3,0.3) + theme(legend.position = &quot;bottom&quot;) bl + bll + plot_annotation(title = &quot;Density plots of Polarity for All Topics&quot;, theme = theme(plot.title = element_text(size=18))) 16.3.2 Subjectivity all_nyt &lt;- df_all %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% group_by(dominant_topic) %&gt;% summarize(mean_subjectivity = mean(subjectivity)) %&gt;% arrange(desc(mean_subjectivity)) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)), ) %&gt;% ggplot() + aes(x=reorder(dominant_topic_name, mean_subjectivity), y=mean_subjectivity) + geom_bar(stat = &quot;identity&quot;, fill = RColorBrewer::brewer.pal(7, &quot;Set2&quot;)) + coord_flip() + labs(x= &quot;Topic&quot;, y=&quot;Mean polarity&quot;, title = &quot;New York Times&quot;) + theme_minimal() + ylim(0,0.5) all_guardian &lt;- df_all %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% group_by(dominant_topic) %&gt;% summarize(mean_subjectivity = mean(subjectivity)) %&gt;% arrange(desc(mean_subjectivity)) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)), ) %&gt;% ggplot() + aes(x=reorder(dominant_topic_name, mean_subjectivity), y=mean_subjectivity) + geom_bar(stat = &quot;identity&quot;, fill = RColorBrewer::brewer.pal(7, &quot;Set1&quot;)) + coord_flip() + labs(x= &quot;Topic&quot;, y=&quot;Mean polarity&quot;, title = &quot;The Guardian&quot;) + theme_minimal() + ylim(0,0.5) all_nyt/all_guardian + plot_annotation(title = &quot;Mean subjectivity for Each Topic&quot;, theme = theme(plot.title = element_text(size=18))) bl &lt;- df_all %&gt;% filter(newspaper == &quot;New York Times&quot;) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)) ) %&gt;% ggplot() + aes(x=subjectivity, fill = dominant_topic_name) + geom_density(alpha = 0.5) + theme_minimal() + scale_fill_manual(values = RColorBrewer::brewer.pal(7, &quot;Set2&quot;), name = &quot;&quot;) + labs(x=&quot;Polarity&quot;, y=&quot;Density&quot;, title=&quot;New York Times&quot;) + facet_wrap(~dominant_topic_name, nrow=7) + xlim(0,0.6) + theme(legend.position = &quot;bottom&quot;) bll &lt;- df_all %&gt;% filter(newspaper == &quot;The Guardian&quot;) %&gt;% mutate( dominant_topic = as.factor(dominant_topic), dominant_topic_name = paste(&quot;Topic&quot;, as.character(dominant_topic)) ) %&gt;% ggplot() + aes(x=subjectivity, fill = dominant_topic_name) + geom_density(alpha = 0.5) + theme_minimal() + scale_fill_manual(values = RColorBrewer::brewer.pal(7, &quot;Set1&quot;), name = &quot;&quot;) + labs(x=&quot;Subjectivity&quot;, y=&quot;Density&quot;, title=&quot;The Guardian&quot;) + facet_wrap(~dominant_topic_name, nrow=7) + xlim(0,0.6) + theme(legend.position = &quot;bottom&quot;) bl + bll + plot_annotation(title = &quot;Density plots of Subjectivity for All Topics&quot;, theme = theme(plot.title = element_text(size=18))) References "],["miscellaneous-plots.html", "Chapter 17 Miscellaneous Plots", " Chapter 17 Miscellaneous Plots Here I will plot the number of articles published each month. I load packages pacman::p_load(tidyverse, wesanderson) I load the datasets df_nyt &lt;- read_csv(&quot;data/new_york_times/data_additional/NYT_clean_cp2.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------------------------------------------- ## cols( ## headline = col_character(), ## url = col_character(), ## date = col_date(format = &quot;&quot;) ## ) df_guardian &lt;- read_csv(&quot;data/guardian/data_additional/guardian_clean_cp2.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------------------------------------------- ## cols( ## headline = col_character(), ## url = col_character(), ## date = col_date(format = &quot;&quot;) ## ) Here I quickly define some color palettes with colors that i like. These palettes will be used for different plots. The same colors will be used consistently throughout the notebook. color_palette_newspaper &lt;- c(wes_palette(&quot;Chevalier1&quot;)[1], wes_palette(&quot;Darjeeling2&quot;)[2]) I do some wrangling. df_nyt &lt;- df_nyt %&gt;% mutate(newspaper = &quot;New York Times&quot;, month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) df_guardian &lt;- df_guardian %&gt;% mutate(newspaper = &quot;The Guardian&quot;, month = lubridate::floor_date(date, &quot;month&quot;), month = as.Date(month)) df_both &lt;- rbind(df_nyt, df_guardian) df_both &lt;- as_tibble(df_both) df_both &lt;- df_both %&gt;% group_by(month, newspaper) %&gt;% tally() %&gt;% rename(sum_articles_month = n) Plotting. q &lt;- df_both %&gt;% ggplot() + aes(x=month, color = newspaper) + #geom_smooth(method = &quot;gam&quot;, aes(y=sum_articles_month), size = 1.2) + geom_line(aes(y=sum_articles_month), size = 1.2) + geom_point(aes(y=sum_articles_month), size = 0.7) + scale_x_date(limits = as.Date(c(&quot;1996-01-01&quot;, &quot;2021-11-01&quot;)), breaks = &quot;2 years&quot;, date_labels = &quot;%Y&quot;) + facet_wrap(~newspaper, nrow = 2) + labs(x=&quot;Date&quot;, y = &quot;Sum of Articles&quot;, title = &quot;Timeline of the Number of Articles Published&quot;) + theme_minimal() + scale_color_manual(values = color_palette_newspaper, name = &quot;&quot;) + theme(legend.position=&quot;none&quot;) q "],["gender-dataset-for-further-analysis.html", "Chapter 18 Gender Dataset for further analysis 18.1 Dataset with authors 18.2 Comparing the datasets 18.3 Adding gender information to NYT_aut", " Chapter 18 Gender Dataset for further analysis div.red { background-color:#e06666; border-radius: 5px; padding: 20px;} This Chapter is not used for the project, but it can be used for further analysis. Here i will make another dataset from NYT_raw which also contains the name of the authors and their gender. This dataset called NYT_clean_author is not used for analysis in this project, but is available for analysis in another project for another time maybe. It is necessary to make an additional dataset because not all articles has information about the authors. So NYT_clean_author will contain less articles than NYT_clean because articles with missing author information is dropped from the dataset. 18.1 Dataset with authors First thing first, we start by loading the packages tidyverse (Wickham et al. 2019), DT (Xie, Cheng, and Tan 2021) and gender (Mullen 2020). pacman::p_load(tidyverse, gender, DT) We are going to make the author dataset NYT_clean_author from the dataset NYT_raw. It mostly follows the syntax from the package tidyverse. #selecting coloumns to keep coloumns_to_select &lt;- c(&quot;response.docs.headline.main&quot;, &quot;response.docs.web_url&quot;, &quot;response.docs.pub_date&quot;, &quot;firstname&quot;, &quot;lastname&quot;, &quot;rank&quot;,) #creating the new dataframe NYT_aut &lt;- NYT_raw %&gt;% # Information about the name of the author is stored inside a dataframe in a column of the dataframe all_articles. I use the function &quot;unnest&quot; to unpack the dataframe into separate columns unnest(cols = response.docs.byline.person) %&gt;% #selecting the defined columns select(coloumns_to_select) %&gt;% #renaming columns to more humane names rename( &quot;headline&quot; = &quot;response.docs.headline.main&quot;, &quot;url&quot; = &quot;response.docs.web_url&quot;, &quot;date&quot; = &quot;response.docs.pub_date&quot; ) %&gt;% #making a new coloumn with full name mutate( full_name = str_c(firstname, lastname, sep = &quot;_&quot;) ) %&gt;% #formating coloumns to the correct class mutate( date = as.Date(date), firstname = as.factor(firstname), lastname = as.factor(lastname), rank = as.factor(rank), full_name = as.factor(full_name) ) %&gt;% #filtering rows where author is missing filter(is.na(full_name) == F) %&gt;% #arranging by date so that the articles are in chronological order arrange(by=date) #saving to a csv write_csv(NYT_aut, &quot;data/new_york_times/data_additional/NYT_clean_author_cp18.csv&quot;) Now lets inspect the cleaned dataframe. to see if it looks allright. Again, we use the function datatable from the package DT. The cleaned dataset contains the following columns. headline which is the title/headline of the article url which is a url leading to the the article on the NYT webpage date which is the publication date firstname which is the first name of the author of the article lastname which is the last name of the author of the article rank which indicates the rank of the author. full_name which is the full name of the author #reading the csv NYT_aut &lt;- read_csv(&quot;data/author_dataset/NYT_author.csv&quot;) #making a nice dataframe that we can browse. Note that we remove abstract because there is too much text in it to show in a nice way. font.size &lt;- &quot;8pt&quot; DT::datatable( NYT_aut, rownames = FALSE, filter = &quot;top&quot;, options = list( initComplete = htmlwidgets::JS( &quot;function(settings, json) {&quot;, paste0(&quot;$(this.api().table().container()).css({&#39;font-size&#39;: &#39;&quot;, font.size, &quot;&#39;});&quot;), &quot;}&quot;), pagelength = 3, scrollX=T, autoWidth = TRUE ) ) This dataset NYT_aut has another structure than NYT_clean. In NYT_aut each row corresponds to an author instead of an article. This means that an article has x number of rows for x number of authors to that article. The column rank indicates the rank of each author, so that an article with 2 authors will have 2 rows with 1 and 2 as values in rank. This is because the function unnest makes x number of observations for x number of authors. You can see it for yourself in the table below. #dropping abstract and snippet since they make it hard to read the data NYT_aut %&gt;% #filtering rows where rank=1 is followed by rank=2 and where rank &gt; 1 filter(lead(rank) == 2 | rank &gt; 1) %&gt;% #arranging by date arrange(url) %&gt;% #making a datatable again DT::datatable( rownames = FALSE, filter = &quot;top&quot;, options = list( initComplete = htmlwidgets::JS( &quot;function(settings, json) {&quot;, paste0(&quot;$(this.api().table().container()).css({&#39;font-size&#39;: &#39;&quot;, font.size, &quot;&#39;});&quot;), &quot;}&quot;), pagelength = 3, scrollX=T, autoWidth = TRUE ) ) 18.2 Comparing the datasets Lastly, we are going to do a quick comparison of the datasets NYT_clean and NYT_aut. We start by loading `` df_NYT &lt;- read_csv(&quot;data/new_york_times/data_additional/NYT_clean_cp2.csv&quot;) ## ## -- Column specification -------------------------------------------------------------------------------------------------------------------- ## cols( ## headline = col_character(), ## url = col_character(), ## date = col_date(format = &quot;&quot;) ## ) We start by comparing the number of articles in each dataset. #summing articles in df_nyt nrow(df_NYT) ## [1] 23502 #summing articles in NYT_aut NYT_aut %&gt;% filter(rank == 1) %&gt;% nrow() ## [1] 18417 We see that df_NYT has 23.502 articles similar to data_raw and NYT_aut has 18.417 articles meaning that ~7000 articles has been dropped in the author dataset. This is the reason why we work with two datasets instead of one. In the next chunk we see of there are specific dates where articles from NYT_aut has been dropped. In other words if there is an unbalance in the dates of the author dataset. #defining a dataset from df_NYT containing only dates and a indicator x1 &lt;- df_NYT %&gt;% select(date) %&gt;% mutate( Dataset = &quot;Full Dataset&quot; ) #defining a dataset from NYT_aut containing only dates and a indicator x2 &lt;- NYT_aut %&gt;% filter(rank == 1) %&gt;% select(date) %&gt;% mutate( Dataset = &quot;Dataset with authors&quot; ) #contatinating the two datasets and setting data to factor xx &lt;- as_tibble(rbind(x1, x2)) %&gt;% mutate( Dataset = as.factor(Dataset) ) #relevelling Dataset xx$Dataset &lt;- relevel(xx$Dataset, &quot;Full Dataset&quot;) #we sum together articles for each date and make a plot for each dataset xx %&gt;% group_by(date, Dataset) %&gt;% tally() %&gt;% ggplot() + aes(x=date, y=n, color = Dataset) + geom_line() + labs(y = &quot;hits&quot;) Figure 18.1: Number of articles published each day in the full dataset and in the dataset with authors. By eyeballing figure 18.1 we see that NYT_aut is missing articles at a fairly evenly rate across all dates. In other words there are not specific periods where NYT_aut drops completely in articles. The ~7000 articles that are missing from df_NYT are spread across the whole period. 18.3 Adding gender information to NYT_aut Next up we are going to add a gender to each author (M/F). I found a package named gender which contains lists of male and female names. We can compare the names in this dictionary to the name of the authors in our dataset and assign each author a gender. First we reformat the dataframe a little bit. #reformatting the dataframe so it can enter the &quot;gender_df&quot; function below NYT_aut &lt;- NYT_aut %&gt;% mutate( firstname = as.character(firstname), year = 2012 ) Then we use the function gender_df to predict the gender of all the authors in our dataset. #the function gender_df predicts gender using a coloumn of names from a dataframe gender_basic &lt;- gender_df( NYT_aut, name_col = &quot;firstname&quot;, year_col = &quot;year&quot; ) Then we merge the dataframes gender_basic and NYT_aut. This is a little messy but it works. #renaming the column containing names so it can merge back with df gender_basic &lt;- gender_basic %&gt;% rename(firstname = name) #joining the two dataframes &quot;NYT_aut&quot; and &quot;gender_basic&quot;. NYT_aut contains duplicates of #author, since each author wrote several articles in the period where the data #was collected. We deal with these duplicates by adding a row number, so #each author gets two keys to join by. NYT_aut &lt;- left_join(NYT_aut %&gt;% group_by(firstname) %&gt;% mutate(id = row_number()), gender_basic %&gt;% group_by(firstname) %&gt;% mutate(id = row_number()), by = c(&quot;firstname&quot;, &quot;id&quot;)) Finally we remove the extra columns created by the function gender_df which we dont need. #removing the extra columns created by the function &quot;gender_df&quot; coloumns_to_remove = c(&quot;year&quot;, &quot;id&quot;, &quot;proportion_male&quot;, &quot;proportion_female&quot;, &quot;year_min&quot;, &quot;year_max&quot;) NYT_aut &lt;- NYT_aut %&gt;% select(-(coloumns_to_remove)) We save the dataframe write_csv(NYT_aut, &quot;data/new_york_times/data_additional/NYT_clean_author_cp18.csv&quot;) Now we have successfully assigned a gender to each author. Lets inspect this. Lets see how many male, female and missing authors there are in the dataset. #checking how many males, females and NA&#39;s there are NYT_aut %&gt;% #removing duplicates distinct(full_name, .keep_all = TRUE) %&gt;% group_by(gender) %&gt;% summarize(count = n()) %&gt;% knitr::kable(caption = &quot;The number of authors by gender&quot;, col.names = c(&quot;Gender&quot;, &quot;Count&quot;)) Table 18.1: The number of authors by gender Gender Count female 764 male 1610 NA 370 Table 18.1 shows that there are 764 female authors and 1610 male authors, meaning that there are approximately 2 times male authors compared to female authors. This is quite the difference. Moreoever, there are 370 NAs meaning that the names of 370 authors could not be found in the database from the package gender. What is the deal with these NAs? Lets see which author names cannot be assigned to a gender (M/F). NYT_aut %&gt;% distinct(full_name, .keep_all = TRUE) %&gt;% filter(is.na(gender)) %&gt;% select(firstname, full_name) %&gt;% head(15) %&gt;% knitr::kable(caption = &quot;The firstname and full names of authors where the gender-package failed to assign a gender&quot;, col.names = c(&quot;First name&quot;, &quot;Full Name&quot;)) Table 18.2: The firstname and full names of authors where the gender-package failed to assign a gender First name Full Name Barnett Barnett_Rubin International International_Tribune Milt Milt_Bearden Tunku Tunku_Varadarajan Barth Barth_Healey Was Was_Mr Geoff Geoff_Nicholson Katha Katha_Pollitt Pankaj Pankaj_Mishra C. C._Chivers Thom Thom_Shanker R. R._Apple Javaid Javaid_Khan From From_WIRE Somini Somini_Sengupta Here we can see the firstname of authors which the gender-package failed to recognize. These seem like the names of immigrants and some weird names. Some more gender stuff over time NYT_aut %&gt;% group_by(date, gender) %&gt;% tally() %&gt;% ggplot() + aes(x=date, y=n, color = gender) + geom_line() + labs(y = &quot;hits&quot;) + facet_wrap(~gender, nrow = 3) References "],["references.html", "Chapter 19 References", " Chapter 19 References Here are all the R-packages used for the project. "]]
